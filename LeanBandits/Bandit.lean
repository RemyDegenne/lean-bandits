/-
Copyright (c) 2025 Rémy Degenne. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: Rémy Degenne, Paulo Rauber
-/
import Mathlib
import LeanBandits.Algorithm
import LeanBandits.ForMathlib.CondDistrib
import LeanBandits.ForMathlib.KernelCompositionLemmas
import LeanBandits.ForMathlib.Traj

/-!
# Bandit
-/

open MeasureTheory ProbabilityTheory Filter Real Finset Learning

open scoped ENNReal NNReal

namespace Bandits

variable {α R : Type*} {mα : MeasurableSpace α} {mR : MeasurableSpace R}

section MeasureSpace

namespace Bandit

/-- Kernel describing the distribution of the next arm-reward pair given the history up to `n`. -/
noncomputable
def stepKernel (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] (n : ℕ) :
    Kernel (Iic n → α × R) (α × R) :=
  Learning.stepKernel alg (stationaryEnv ν) n
deriving IsMarkovKernel

@[simp]
lemma fst_stepKernel (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] (n : ℕ) :
    (stepKernel alg ν n).fst = alg.policy n := by
  rw [stepKernel, Learning.fst_stepKernel]

@[simp]
lemma snd_stepKernel (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] (n : ℕ) :
    (stepKernel alg ν n).snd = ν ∘ₖ alg.policy n := by
  rw [stepKernel, Learning.stepKernel, stationaryEnv_feedback, Kernel.snd_compProd_prodMkLeft]

/-- Measure on the sequence of arms pulled and rewards observed generated by the bandit. -/
noncomputable
def trajMeasure (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] : Measure (ℕ → α × R) :=
  Kernel.trajMeasure (alg.p0 ⊗ₘ ν) (stepKernel alg ν)
deriving IsProbabilityMeasure

/-- Measure of an infinite stream of rewards from each arm. -/
noncomputable
def streamMeasure (ν : Kernel α R) [IsMarkovKernel ν] : Measure (ℕ → α → R) :=
  Measure.infinitePi fun _ ↦ Measure.infinitePi ν
deriving IsProbabilityMeasure

/-- Joint distribution of the sequence of arm pulled and rewards, and a stream of independent
rewards from all arms. -/
noncomputable
def measure (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] :
    Measure ((ℕ → α × R) × (ℕ → α → R)) :=
  (trajMeasure alg ν).prod (streamMeasure ν)
deriving IsProbabilityMeasure

@[simp]
lemma fst_measure (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] :
    (measure alg ν).fst = trajMeasure alg ν := by
  rw [measure, Measure.fst_prod]

@[simp]
lemma snd_measure (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] :
    (measure alg ν).snd = streamMeasure ν := by
  rw [measure, Measure.snd_prod]

end Bandit

/-- `arm n` is the arm pulled at time `n`. This is a random variable on the measurable space
`ℕ → α × ℝ`. -/
def arm (n : ℕ) (h : ℕ → α × R) : α := (h n).1

/-- `reward n` is the reward at time `n`. This is a random variable on the measurable space
`ℕ → α × R`. -/
def reward (n : ℕ) (h : ℕ → α × R) : R := (h n).2

/-- `hist n` is the history up to time `n`. This is a random variable on the measurable space
`ℕ → α × R`. -/
def hist (n : ℕ) (h : ℕ → α × R) : Iic n → α × R := fun i ↦ h i

@[fun_prop]
lemma measurable_arm (n : ℕ) : Measurable (arm n (α := α) (R := R)) := measurable_action n

@[fun_prop]
lemma measurable_arm_prod : Measurable (fun p : ℕ × (ℕ → α × R) ↦ arm p.1 p.2) :=
  measurable_action_prod

@[fun_prop]
lemma measurable_reward (n : ℕ) : Measurable (reward n (α := α) (R := R)) :=
  Learning.measurable_reward n

@[fun_prop]
lemma measurable_reward_prod : Measurable (fun p : ℕ × (ℕ → α × R) ↦ reward p.1 p.2) :=
  Learning.measurable_reward_prod

@[fun_prop]
lemma measurable_hist (n : ℕ) : Measurable (hist n (α := α) (R := R)) :=
  Learning.measurable_hist n

lemma hist_eq_frestrictLe :
    hist = Preorder.frestrictLe («π» := fun _ ↦ α × R) := by
  ext n h i : 3
  simp [hist, Preorder.frestrictLe]

/-- Filtration of the bandit process. -/
protected def filtration (α R : Type*) [MeasurableSpace α] [MeasurableSpace R] :
    Filtration ℕ (inferInstance : MeasurableSpace (ℕ → α × R)) :=
  MeasureTheory.Filtration.piLE (X := fun _ ↦ α × R)

lemma condDistrib_arm_reward [StandardBorelSpace α] [Nonempty α] [StandardBorelSpace R] [Nonempty R]
    (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] (n : ℕ) :
    condDistrib (fun h ↦ (arm (n + 1) h, reward (n + 1) h)) (hist n) (Bandit.trajMeasure alg ν)
      =ᵐ[(Bandit.trajMeasure alg ν).map (hist n)] Bandit.stepKernel alg ν n :=
  Learning.condDistrib_step alg (stationaryEnv ν) n

lemma condDistrib_reward' [StandardBorelSpace α] [Nonempty α] [StandardBorelSpace R] [Nonempty R]
    (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] (n : ℕ) :
    condDistrib (reward (n + 1)) (fun ω ↦ (hist n ω, arm (n + 1) ω)) (Bandit.trajMeasure alg ν)
      =ᵐ[(Bandit.trajMeasure alg ν).map (fun ω ↦ (hist n ω, arm (n + 1) ω))] ν.prodMkLeft _ :=
  Learning.condDistrib_reward alg (stationaryEnv ν) n

lemma condDistrib_reward [StandardBorelSpace α] [Nonempty α] [StandardBorelSpace R] [Nonempty R]
    (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] (n : ℕ) :
    condDistrib (reward n) (arm n) (Bandit.trajMeasure alg ν)
      =ᵐ[(Bandit.trajMeasure alg ν).map (arm n)] ν := by
  cases n with
  | zero => sorry
  | succ n =>
    have h_ar := condDistrib_arm_reward alg ν n
    have h_prod := condDistrib_prod_left (X := arm (n + 1)) (Y := reward (n + 1))
      (T := hist n) (μ := Bandit.trajMeasure alg ν) (by fun_prop) (by fun_prop) (by fun_prop)
    sorry

lemma condDistrib_arm [StandardBorelSpace α] [Nonempty α] [StandardBorelSpace R] [Nonempty R]
    (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] (n : ℕ) :
    condDistrib (arm (n + 1)) (hist n) (Bandit.trajMeasure alg ν)
      =ᵐ[(Bandit.trajMeasure alg ν).map (hist n)] alg.policy n :=
  Learning.condDistrib_action alg (stationaryEnv ν) n

lemma hasLaw_step_zero (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] :
    HasLaw (fun h : ℕ → α × R ↦ h 0) (alg.p0 ⊗ₘ ν) (Bandit.trajMeasure alg ν) :=
  Learning.hasLaw_step_zero alg (stationaryEnv ν)

lemma hasLaw_arm_zero
    (alg : Algorithm α R) (ν : Kernel α R) [IsMarkovKernel ν] :
    HasLaw (arm 0) alg.p0 (Bandit.trajMeasure alg ν) :=
  Learning.hasLaw_action_zero alg (stationaryEnv ν)

/-- The reward at time `n+1` is independent of the history up to time `n` given the arm at `n+1`. -/
lemma condIndepFun_reward_hist_arm [StandardBorelSpace α] [Nonempty α]
    [StandardBorelSpace R] [Nonempty R]
    {alg : Algorithm α R} {ν : Kernel α R} [IsMarkovKernel ν] (n : ℕ) :
    CondIndepFun (MeasurableSpace.comap (arm (n + 1)) inferInstance)
      (measurable_arm _).comap_le (reward (n + 1)) (hist n) (Bandit.trajMeasure alg ν) := by
  rw [condIndepFun_iff_condDistrib_prod_ae_eq_prodMkLeft (by fun_prop) (by fun_prop) (by fun_prop)]
  sorry

section DetAlgorithm

variable {nextArm : (n : ℕ) → (Iic n → α × R) → α} {h_next : ∀ n, Measurable (nextArm n)}
  {arm0 : α} {ν : Kernel α R} [IsMarkovKernel ν]

lemma HasLaw_arm_zero_detAlgorithm :
    HasLaw (arm 0) (Measure.dirac arm0)
      (Bandit.trajMeasure (detAlgorithm nextArm h_next arm0) ν) where
  map_eq := (hasLaw_arm_zero _ _).map_eq

lemma arm_zero_detAlgorithm [MeasurableSingletonClass α] :
    arm 0 =ᵐ[Bandit.trajMeasure (detAlgorithm nextArm h_next arm0) ν] fun _ ↦ arm0 := by
  have h_eq : ∀ᵐ x ∂((Bandit.trajMeasure (detAlgorithm nextArm h_next arm0) ν).map (arm 0)), x
      = arm0 := by
    rw [(hasLaw_arm_zero _ _).map_eq]
    simp [detAlgorithm]
  exact ae_of_ae_map (by fun_prop) h_eq

lemma arm_detAlgorithm_ae_eq (n : ℕ) :
    arm (n + 1) =ᵐ[Bandit.trajMeasure (detAlgorithm nextArm h_next arm0) ν]
      fun h ↦ nextArm n (fun i ↦ h i) := by
  sorry

example [MeasurableSingletonClass α] :
    ∀ᵐ h ∂(Bandit.trajMeasure (detAlgorithm nextArm h_next arm0) ν),
    arm 0 h = arm0 ∧ ∀ n, arm (n + 1) h = nextArm n (fun i ↦ h i) := by
  rw [eventually_and, ae_all_iff]
  exact ⟨arm_zero_detAlgorithm, arm_detAlgorithm_ae_eq⟩

end DetAlgorithm

end MeasureSpace

end Bandits
