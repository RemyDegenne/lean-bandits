\chapter{Stochastic multi-armed bandits}

\section{Bandit model and probability space}


\begin{definition}[Bandit]\label{def:bandit}
  \leanok
  \lean{Bandits.Bandit}
The interaction of an algorithm with a stochastic bandit with arms in $\mathcal{A}$ (a measurable space) and real rewards is described by the following data:
\begin{itemize}
  \item $\nu : \mathcal{A} \rightsquigarrow \mathbb{R}$, a Markov kernel, conditional distribution of the rewards given the arm pulled,
  \item for all $t \in \mathbb{N}$, a policy $\pi_t : (\mathcal{A} \times \mathbb{R})^{t+1} \rightsquigarrow \mathcal{A}$, a Markov kernel which gives the distribution of the arm to pull at time $t+1$ given the history of previous pulls and rewards,
  \item $P_0 \in \mathcal{P}(\mathcal{A})$, a probability measure that gives the distribution of the first arm to pull.
\end{itemize}
\end{definition}


\begin{definition}[Bandit probability space]\label{def:Bandit.measure}
  \uses{def:bandit}
  \leanok
  \lean{Bandits.Bandit.measure}
By an application of the Ionescu-Tulcea theorem, a bandit $\mathcal{B} = (\nu, \pi, P_0)$ defines a probability distribution on the space $\Omega := (\mathcal{A} \times \mathbb{R})^{\mathbb{N}}$, the space of infinite sequences of arms and rewards.
We denote that distribution by $\mathbb{P}$.
TODO: explain how the probability distribution is constructed.
\end{definition}


\begin{definition}[Arms, rewards and history]\label{def:armAndReward}
  \leanok
  \lean{Bandits.arm, Bandits.reward, Bandits.hist}
For $t \in \mathbb{N}$, we denote by $A_t$ the arm pulled at time $t$ and by $X_t$ the reward received at time $t$.
Formally, these are measurable functions on $\Omega = (\mathcal{A} \times \mathbb{R})^{\mathbb{N}}$, defined by $A_t(\omega) = \omega_{t,1}$ and $X_t(\omega) = \omega_{t,2}$.
We denote by $H_t \in (\mathcal{A} \times \mathbb{R})^{t+1}$ the history of pulls and rewards up to and including time $t$, that is $H_t = ((A_0, X_0), \ldots, (A_t, X_t))$.
Formally, $H_t(\omega) = (\omega_0, \ldots, \omega_t)$.
\end{definition}


\begin{lemma}\label{lem:condDistrib_reward}
  \uses{def:Bandit.measure,def:armAndReward}
  \leanok
  \lean{Bandits.condDistrib_reward}
The conditional distribution of the reward $X_t$ given the arm $A_t$ in the bandit probability space $(\Omega, \mathbb{P})$ is $\nu(A_t)$.
\end{lemma}

\begin{proof}

\end{proof}


\begin{lemma}\label{lem:law_arm_zero}
  \uses{def:Bandit.measure,def:armAndReward}
The law of the arm $A_0$ in the bandit probability space $(\Omega, \mathbb{P})$ is $P_0$.
\end{lemma}

\begin{proof}

\end{proof}


\begin{lemma}\label{lem:condDistrib_arm}
  \uses{def:Bandit.measure,def:armAndReward}
  \leanok
  \lean{Bandits.condDistrib_arm}
The conditional distribution of the arm $A_{t+1}$ given the history $H_t$ in the bandit probability space $(\Omega, \mathbb{P})$ is $\pi_t(H_t)$.
\end{lemma}

\begin{proof}

\end{proof}


\section{Regret and other bandit quantities}

\begin{definition}\label{def:armMean}
  \uses{def:bandit}
  \leanok % no actual Lean def, but we don't need one
For an arm $a \in \mathcal{A}$, we denote by $\mu_a$ the mean of the rewards for that arm, that is $\mu_a = \nu(a)[X]$.
We denote by $\mu^*$ the mean of the best arm, that is $\mu^* = \max_{a \in \mathcal{A}} \mu_a$.
\end{definition}


\begin{definition}[Regret]\label{def:regret}
  \uses{def:armMean}
  \leanok
  \lean{Bandits.regret}
The regret $R_T$ of a sequence of arms $A_0, \ldots, A_{T-1}$ after $T$ pulls is the difference between the cumulative reward of always playing the best arm and the cumulative reward of the sequence:
\begin{align*}
  R_T = T \mu^* - \sum_{t=0}^{T-1} \mu_{A_t} \: .
\end{align*}
\end{definition}


\begin{definition}\label{def:gap}
  \uses{def:armMean}
  \leanok
  \lean{Bandits.gap}
For an arm $a \in \mathcal{A}$, its gap is defined as the difference between the mean of the best arm and the mean of that arm: $\Delta_a = \mu^* - \mu_a$.
\end{definition}


\begin{definition}\label{def:pullCount}
  \uses{def:bandit}
  \leanok
  \lean{Bandits.pullCount}
For an arm $a \in \mathcal{A}$ and a time $t \in \mathbb{N}$, we denote by $N_{t,a}$ the number of times that arm $a$ has been pulled before time $t$, that is $N_{t,a} = \sum_{s=0}^{t-1} \mathbb{I}\{A_s = a\}$.
\end{definition}


\begin{lemma}\label{lem:regret_eq_sum_pullCount_mul_gap}
  \uses{def:regret,def:gap,def:pullCount}
  \leanok
  \lean{Bandits.regret_eq_sum_pullCount_mul_gap}
For $\mathcal{A}$ finite, the regret $R_T$ can be expressed as a sum over the arms and their gaps:
\begin{align*}
  R_T = \sum_{a \in \mathcal{A}} N_{T,a} \Delta_a \: .
\end{align*}
\end{lemma}

\begin{proof}
  \leanok
\begin{align*}
  R_T = T \mu^* - \sum_{t=0}^{T-1} \mu_{A_t}
  &= T \mu^* - \sum_{a \in \mathcal{A}} \sum_{t=0}^{T-1} \mathbb{I}\{A_t = a\} \mu_a
  \\
  &= T \mu^* - \sum_{a \in \mathcal{A}} N_{T,a} \mu_a
  \\
  &= \sum_{a \in \mathcal{A}} N_{T,a} \mu^* - \sum_{a \in \mathcal{A}} N_{T,a} \mu_a
  \\
  &= \sum_{a \in \mathcal{A}} N_{T,a} \Delta_a
  \: .
\end{align*}
\end{proof}


\section{Alternative model}

The description of the bandit model above considers that at time $t$, a reward $X_t$ is generated, depending on the arm $A_t$ pulled at that time.
An alternative way to talk about that process is to imagine that there is a stream of rewards from each arm, and that the algorithm sees the first, then second, etc. reward from the arms at it pulls them.
We introduce definitions to talk about the $n^{th}$ reward of an arm, and the time at which that reward is pulled.

\begin{definition}\label{def:timeOfPull}
  \uses{def:pullCount}
  \leanok
  \lean{Bandits.stepsUntil}
For an arm $a \in \mathcal{A}$ and a time $n \in \mathbb{N}$, we denote by $T_{n,a}$ the time at which arm $a$ was pulled for the $n$-th time, that is $T_{n,a} = \min\{s \in \mathbb{N} \mid N_{s+1,a} = n\}$.
Note that $T_{n, a}$ can be infinite if the arm is not pulled $n$ times.
\end{definition}


\begin{definition}\label{def:altReward}
  \uses{def:timeOfPull}
  \leanok
  \lean{Bandits.rewardByCount}
For $a \in \mathcal{A}$ and $n \in \mathbb{N}$, let $Z_{n,a} \sim \nu(a)$, independent of everything else.
We define $Y_{n, a} = X_{T_{n,a}} \mathbb{I}\{T_{n, a} < \infty\} + Z_{n,a} \mathbb{I}\{T_{n, a} = \infty\}$, the reward received when pulling arm $a$ for the $n$-th time if that time is finite, and equal to $Z_{n,a}$ otherwise.
\end{definition}

TODO: that definition requires changing the probability space to $\Omega \times \mathbb{R}^{\mathbb{N} \times \mathcal{A}}$.

\begin{lemma}\label{lem:iid_altReward}
  \uses{def:altReward}
The rewards $(Y_{n,a})_{n \in \mathbb{N}}$ are independent and identically distributed random variables, with distribution $\nu(a)$.
\end{lemma}

\begin{proof}

\end{proof}


\begin{lemma}\label{lem:independent_altReward}
  \uses{def:altReward}
For $a \in \mathcal{A}$, let $Y^{(a)} = (Y_{n,a})_{n \in \mathbb{N}} \in \mathbb{R}^{\mathbb{N}}$ be the sequence of rewards obtained from pulling arm $a$. Then the sequences $(Y^{(a)})_{a \in \mathcal{A}}$ are independent.
\end{lemma}

\begin{proof}

\end{proof}


\begin{lemma}\label{lem:timeOfPull_pullCount_le}
  \uses{def:timeOfPull,def:pullCount}
  \leanok
  \lean{Bandits.stepsUntil_pullCount_le}
$T_{N_{t+1, a}, a} \le t < \infty$ for all $t \in \mathbb{N}$ and $a \in \mathcal{A}$.
\end{lemma}

\begin{proof}\leanok
By definition, $T_{N_{t,a}, a} = \min\{s \in \mathbb{N} \mid N_{s+1,a} = N_{t,a}\} \le t - 1 < \infty$.
\end{proof}


\begin{lemma}\label{lem:timeOfPull_pullCount_eq}
  \uses{def:timeOfPull,def:pullCount}
  \leanok
  \lean{Bandits.stepsUntil_pullCount_eq}
$T_{N_{t+1, A_t}, A_t} = t$ for all $t \in \mathbb{N}$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:altReward_pullCount}
  \uses{def:altReward,def:pullCount}
  \leanok
  \lean{Bandits.rewardByCount_pullCount_add_one_eq_reward}
$Y_{N_{t+1, A_t}, A_t} = X_t$ for all $t \in \mathbb{N}$ and $a \in \mathcal{A}$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:timeOfPull_pullCount_eq}
By Lemma~\ref{lem:timeOfPull_pullCount_eq}, we have $T_{N_{t+1,A_t}, A_t} = t < \infty$, so $Y_{N_{t+1,A_t}, A_t} = X_{T_{N_{t+1,A_t}, A_t}} = X_t$.

\end{proof}


\begin{lemma}\label{lem:sum_altReward}
  \uses{def:altReward,def:pullCount}
  \leanok
  \lean{Bandits.sum_rewardByCount_eq_sum_reward}
\begin{align*}
  \sum_{n=1}^{N_{t, a}} Y_{n, a} = \sum_{s=0}^{t-1} \mathbb{I}\{A_s = a\} X_s
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok

\end{proof}
