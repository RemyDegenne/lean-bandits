/-
Copyright (c) 2025 RÃ©my Degenne. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: RÃ©my Degenne, Paulo Rauber
-/
import LeanBandits.SequentialLearning.Algorithm

/-!
# Stationary environments
-/

open MeasureTheory ProbabilityTheory Filter Real Finset

open scoped ENNReal NNReal

namespace Learning

variable {Î± R : Type*} {mÎ± : MeasurableSpace Î±} {mR : MeasurableSpace R}

/-- A stationary environment, in which the distribution of the next reward depends only on the last
action. -/
@[simps]
def stationaryEnv (Î½ : Kernel Î± R) [IsMarkovKernel Î½] : Environment Î± R where
  feedback _ := Î½.prodMkLeft _
  Î½0 := Î½

variable {alg : Algorithm Î± R} {Î½ : Kernel Î± R} [IsMarkovKernel Î½]

local notation "ğ”“" => trajMeasure alg (stationaryEnv Î½)

/-- The conditional distribution of the reward at time `n` given the action at time `n` is `Î½`. -/
lemma condDistrib_reward_stationaryEnv [StandardBorelSpace Î±] [Nonempty Î±]
    [StandardBorelSpace R] [Nonempty R] (n : â„•) :
    condDistrib (reward n) (action n) ğ”“ =áµ[(ğ”“).map (action n)] Î½ := by
  cases n with
  | zero =>
    rw [condDistrib_ae_eq_iff_measure_eq_compProd _ (by fun_prop)]
    change (ğ”“).map (step 0) = (ğ”“).map (action 0) âŠ—â‚˜ Î½
    rw [(hasLaw_action_zero alg (stationaryEnv Î½)).map_eq,
      (hasLaw_step_zero alg (stationaryEnv Î½)).map_eq, stationaryEnv_Î½0]
  | succ n =>
    have h_eq := condDistrib_reward alg (stationaryEnv Î½) n
    rw [condDistrib_ae_eq_iff_measure_eq_compProd _ (by fun_prop)] at h_eq âŠ¢
    have : (ğ”“).map (action (n + 1)) = ((ğ”“).map (fun x â†¦ (hist n x, action (n + 1) x))).snd := by
      rw [Measure.snd_map_prodMk (by fun_prop)]
    simp only [stationaryEnv_feedback] at h_eq
    rw [this, â† Measure.snd_prodAssoc_compProd_prodMkLeft, â† h_eq,
      Measure.snd_map_prodMk (by fun_prop), Measure.map_map (by fun_prop) (by fun_prop)]
    congr

/-- The reward at time `n + 1` is conditionally independent of the history up to time `n`
given the action at time `n + 1`. -/
lemma condIndepFun_reward_hist_action [StandardBorelSpace Î±] [Nonempty Î±]
    [StandardBorelSpace R] [Nonempty R] (n : â„•) :
    reward (n + 1) âŸ‚áµ¢[action (n + 1), measurable_action _ ; ğ”“] hist n :=
  condIndepFun_of_exists_condDistrib_prod_ae_eq_prodMkLeft
    (by fun_prop) (by fun_prop) (by fun_prop) (condDistrib_reward alg (stationaryEnv Î½) n)

end Learning
