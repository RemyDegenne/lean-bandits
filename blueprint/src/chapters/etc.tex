\chapter{Bandit algorithms}

\section{Round-Robin}

This is not an interesting bandit algorithm per se, but it is used as a subroutine in other algorithms and can be a simple baseline.
This algorithm simply cycles through the arms in order.

\begin{definition}\label{def:roundRobinAlgorithm}
  \uses{def:detAlgorithm,def:algorithm}
  \leanok
  \lean{Bandits.RoundRobin.nextArm, Bandits.roundRobinAlgorithm}
The Round-Robin algorithm is defined as follows: at time $t \in \mathbb{N}$, $A_t = t \mod K$.
\end{definition}


\begin{lemma}\label{lem:pullCount_roundRobinAlgorithm}
  \uses{def:stationaryEnv,def:IsAlgEnvSeq,def:pullCount,def:roundRobinAlgorithm}
  \leanok
  \lean{Bandits.RoundRobin.pullCount_mul}
For the Round-Robin algorithm, for any arm $a \in [K]$, at time $Km$ we have
\begin{align*}
  N_{Km,a}
  &= m
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  \uses{def:environment,def:detAlgorithm,def:algorithm,def:history,lem:pullCount_basic,def:roundRobinAlgorithm}

\end{proof}


TODO: regret.




\section{Explore-Then-Commit}

Note: times start at 0 to be consistent with Lean.

Note: we will describe the algorithm by writing $A_t = ...$, but our formal bandit model needs a policy $\pi_t$ that gives the distribution of the arm to pull. What me mean is that $\pi_t$ is a Dirac distribution at that arm.

\begin{definition}[Explore-Then-Commit algorithm]\label{def:etcAlgorithm}
  \uses{def:detAlgorithm,def:algorithm}
  \leanok
  \lean{Bandits.ETC.nextArm, Bandits.etcAlgorithm}
The Explore-Then-Commit (ETC) algorithm with parameter $m \in \mathbb{N}$ is defined as follows:
\begin{enumerate}
  \item for $t < Km$, $A_t = t \mod K$ (pull each arm $m$ times),
  \item compute $\hat{A}_m^* = \arg\max_{a \in [K]} \hat{\mu}_a$, where $\hat{\mu}_a = \frac{1}{m} \sum_{t=0}^{Km-1} \mathbb{I}(A_t = a) X_t$ is the empirical mean of the rewards for arm $a$,
  \item for $t \ge Km$, $A_t = \hat{A}_m^*$ (pull the empirical best arm).
\end{enumerate}
\end{definition}


\begin{lemma}\label{lem:ETC.isAlgEnvSeqUntil_roundRobinAlgorithm}
  \uses{def:stationaryEnv,def:IsAlgEnvSeq,def:etcAlgorithm,def:roundRobinAlgorithm}
  \leanok
  \lean{Bandits.ETC.isAlgEnvSeqUntil_roundRobinAlgorithm}
An algorithm-environment sequence for the Explore-Then-Commit algorithm with parameter $m$ is an algorithm-environment sequence for the Round-Robin algorithm until time $Km - 1$.
That is, ETC plays the same as Round-Robin until time $Km - 1$.
\end{lemma}

\begin{proof}
  \leanok

\end{proof}


\begin{lemma}\label{lem:pullCount_etcAlgorithm}
  \uses{def:stationaryEnv,def:IsAlgEnvSeq,def:pullCount,def:etcAlgorithm}
  \leanok
  \lean{Bandits.ETC.pullCount_of_ge}
For the Explore-Then-Commit algorithm with parameter $m$, for any arm $a \in [K]$ and any time $t \ge Km$, we have
\begin{align*}
  N_{t,a}
  &= m + (t - Km) \mathbb{I}\{\hat{A}_m^* = a\}
  \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  \uses{def:environment,def:detAlgorithm,def:algorithm,def:history,lem:pullCount_basic,def:etcAlgorithm,lem:pullCount_roundRobinAlgorithm,lem:ETC.isAlgEnvSeqUntil_roundRobinAlgorithm}

\end{proof}


\begin{lemma}\label{lem:sumRewards_bestArm_le_of_arm_mul_eq}
  \uses{def:stationaryEnv,def:IsAlgEnvSeq,def:sumRewards,def:etcAlgorithm}
  \leanok
  \lean{Bandits.ETC.sumRewards_bestArm_le_of_arm_mul_eq}
If $\hat{A}_m^* = a$, then we have $S_{Km, a^*} \le S_{Km, a}$.
\end{lemma}

\begin{proof}\leanok
  \uses{def:environment,def:detAlgorithm,def:algorithm,def:history,def:pullCount,def:empMean,def:etcAlgorithm}

\end{proof}


\begin{lemma}\label{lem:prob_etc_error_le_exp}
  \uses{def:stationaryEnv,def:IsAlgEnvSeq,def:subGaussian,def:gap,def:etcAlgorithm}
  \leanok
  \lean{Bandits.ETC.prob_arm_mul_eq_le}
Suppose that $\nu(a)$ is $\sigma^2$-sub-Gaussian for all arms $a \in [K]$.
Then for the Explore-Then-Commit algorithm with parameter $m$, for any arm $a \in [K]$ with $\Delta_a > 0$, we have $P(\hat{A}_m^* = a) \le \exp\left(- \frac{m \Delta_a^2}{4 \sigma^2}\right)$.
\end{lemma}

\begin{proof}\leanok
  \uses{def:environment,lem:probReal_sum_le_sum_streamMeasure,lem:prob_sumRewards_le_sumRewards_le,def:detAlgorithm,def:algorithm,thm:ionescu-tulcea,def:sumRewards,def:history,lem:sumRewards_bestArm_le_of_arm_mul_eq,def:pullCount,def:etcAlgorithm}
By Lemma~\ref{lem:sumRewards_bestArm_le_of_arm_mul_eq},
\begin{align*}
  P(\hat{A}_m^* = a)
  &\le P(S_{Km, a} \ge S_{Km, a^*})
  \: .
\end{align*}
By Lemma~\ref{lem:prob_sumRewards_le_sumRewards_le}, and then the concentration inequality of Lemma~\ref{lem:probReal_sum_le_sum_streamMeasure} we have
\begin{align*}
  P\left(S_{Km, a^*} \le S_{Km, a}\right)
  &\le (\otimes_a \nu(a))^{\otimes \mathbb{N}} \left( \sum_{s=0}^{m-1} \omega_{s, a^*} \le \sum_{s=0}^{m-1} \omega_{s, a} \right)
  \\
  &\le \exp\left( -m \frac{\Delta_a^2}{4 \sigma^2} \right)
  \: .
\end{align*}
\end{proof}


\begin{theorem}\label{thm:regret_etc_le}
  \uses{def:stationaryEnv,def:regret,def:IsAlgEnvSeq,def:subGaussian,def:gap,def:etcAlgorithm}
  \leanok
  \lean{Bandits.ETC.regret_le}
Suppose that $\nu(a)$ is $\sigma^2$-sub-Gaussian for all arms $a \in [K]$.
Then for the Explore-Then-Commit algorithm with parameter $m$, the expected regret after $T$ pulls with $T \ge Km$ is bounded by
\begin{align*}
  P[R_T]
  &\le m \sum_{a=1}^K \Delta_a + (T - Km) \sum_{a=1}^K \Delta_a \exp\left(- \frac{m \Delta_a^2}{4 \sigma^2}\right)
  \: .
\end{align*}
\end{theorem}

\begin{proof}\leanok
  \uses{lem:pullCount_etcAlgorithm,def:environment,def:algorithm,cor:integral_regret_eq_sum_mul,lem:prob_etc_error_le_exp,lem:pullCount_basic,def:pullCount}
By Lemma~\ref{lem:regret_eq_sum_pullCount_mul_gap}, we have $P[R_T] = \sum_{a=1}^K P\left[N_{T,a}\right] \Delta_a$~.
It thus suffices to bound $P[N_{T,a}]$ for each arm $a$ with $\Delta_a > 0$.
It suffices to prove that
\begin{align*}
  P[N_{T,a}]
  &\le m + (T - Km) \exp\left(- \frac{m \Delta_a^2}{4 \sigma^2}\right)
  \: .
\end{align*}
By Lemma~\ref{lem:pullCount_etcAlgorithm},
\begin{align*}
  N_{T,a}
  &= m + (T - Km) \mathbb{I}\{\hat{A}_m^* = a\}
  \: .
\end{align*}
It thus suffices to prove the inequality $P(\hat{A}_m^* = a) \le \exp\left(- \frac{m \Delta_a^2}{4 \sigma^2}\right)$ for $\Delta_a > 0$.
This is done in Lemma~\ref{lem:prob_etc_error_le_exp}.
\end{proof}
