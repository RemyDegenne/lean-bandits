\chapter{Stochastic multi-armed bandits}

\section{Algorithm, bandit and probability space}

A bandit algorithm is an algorithm in the sense of Definition~\ref{def:algorithm}.
We call the actions \emph{arms} and the observations \emph{rewards}.

The first arm pulled by the algorithm is sampled from $P_0$, the arm pulled at time $1$ is sampled from $\pi_0(H_0)$, where $H_0 \in \mathcal{A} \times \mathcal{R}$ is the data of the first arm pulled and the first observation received, and so on.


\begin{definition}[Bandit]\label{def:bandit}
  \uses{def:stationaryEnv}
  \leanok
A stochastic bandit is simply a reward distribution for each arm: a Markov kernel $\nu : \mathcal{A} \rightsquigarrow \mathbb{R}$, conditional distribution of the rewards given the arm pulled.
It is a stationary environment in which the observation space is $\mathcal{R} = \mathbb{R}$.
\end{definition}

An algorithm can interact with a bandit to produce a sequence of arms and rewards: after a time $t$, the history $H_t \in (\mathcal{A} \times \mathcal{R})^{t+1}$ contains the arms pulled and rewards received up to that time,
\begin{itemize}
  \item the algorithm chooses an arm $A_{t+1}$ sampled according to its policy $\pi_t(H_t)$,
  \item the bandit generates a reward $R_{t+1}$ according to the distribution $\nu(A_{t+1})$,
  \item the history is updated to $H_{t+1} = ((A_0, R_0), \ldots, (A_{t+1}, R_{t+1}))$.
\end{itemize}

We now want to define a probability space on which we can study the sequences of arms and rewards, and formulate probabilistic statements about the interaction between the algorithm and the bandit.


\begin{definition}[Bandit probability space]\label{def:Bandit.measure}
  \uses{def:algorithm, def:bandit, def:trajMeasure}
  \leanok
  \lean{Bandits.Bandit.trajMeasure, Bandits.Bandit.measure}
As in Definition~\ref{def:trajMeasure}, an algorithm $(\pi, P_0)$ and bandit $\nu$ together defines a probability distribution $\mathbb{P}_{\mathcal{T}}$ on the space $\Omega_{\mathcal{T}} := (\mathcal{A} \times \mathbb{R})^{\mathbb{N}}$, the space of infinite sequences of arms and rewards.
We augment that probability space with a stream of rewards from each arm, independent of the bandit interaction, to get the probability space $(\Omega, \mathbb{P})$, where $\Omega = \Omega_{\mathcal{T}} \times \mathbb{R}^{\mathbb{N} \times \mathcal{A}}$ and $\mathbb{P} = \mathbb{P}_{\mathcal{T}} \otimes (\bigotimes_{n \in \mathbb{N}, a \in \mathcal{A}} \nu(a))$.
\end{definition}


\section{Alternative models: rewards indexed by time or pull count}\label{sec:alt_model}

The description of the bandit model above considers that at time $t$, a reward $R_t$ is generated, depending on the arm $A_t$ pulled at that time.
An alternative way to talk about that process is to imagine that there is a stream of rewards from each arm, and that the algorithm sees the first, then second, etc. reward from the arms at it pulls them.
This uses the random variables $Y_{n, a}$ defined in Definition~\ref{def:rewardByCount}, which represent the $n^{th}$ reward obtained from arm $a$.
We now describe the distribution of those rewards.

The probability space $\Omega_{\mathcal{T}}$ has been augmented with $\Omega' = \mathbb{R}^{\mathbb{N} \times \mathcal{A}}$, on which we put the product measure $\bigotimes_{n \in \mathbb{N}, a \in \mathcal{A}} \nu(a)$.
With that measure, the law of $Z_{n,a}$ is $\nu(a)$.

Our main goal in this section is to prove that $(Y_{n,a})_{n \in \mathbb{N}, a \in \mathcal{A}}$ and $(Z_{n,a})_{n \in \mathbb{N}, a \in \mathcal{A}}$ are identically distributed.


\begin{lemma}\label{lem:measurable_comap_indicator_stepsUntil_eq}
  \uses{def:stepsUntil}
  \leanok
  \lean{Bandits.measurable_comap_indicator_stepsUntil_eq}
The function $\mathbb{I}\{T_{n,a} = t\} : \Omega \to \{0, 1\}$ is measurable with respect to the sigma-algebra generated by $(H_{t-1}, A_t)$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


% todo: move to the previous section about algorithms, in the stationary env section
\begin{lemma}\label{lem:condIndepFun_reward_hist_arm}
  \uses{def:actionReward, def:history}
  \leanok
  \lean{Bandits.condIndepFun_reward_hist_arm}
$R_{t+1}$ and $H_t$ are conditionally independent given $A_{t+1}$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:condIndepFun_reward_stepsUntil_arm}
  \uses{def:stepsUntil, def:actionReward, def:Bandit.measure}
  \leanok
  \lean{Bandits.condIndepFun_reward_stepsUntil_arm}
For $t > 0$, $R_t$ and $\mathbb{I}\{T_{n, a} = t\}$ are conditionally independent given $A_t$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:measurable_comap_indicator_stepsUntil_eq, lem:condIndepFun_reward_hist_arm}

\end{proof}


\begin{lemma}\label{lem:reward_cond_stepsUntil}
  \uses{def:stepsUntil, def:rewardByCount, def:Bandit.measure}
  \leanok
  \lean{Bandits.reward_cond_stepsUntil}
Let $n > 0$, $t \in \mathbb{N}$ and suppose that $\mathbb{P}(T_{n, a} = t) > 0$.
Then $P[R_t \mid T_{n, a} = t] = \nu(a)$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:condIndepFun_reward_stepsUntil_arm, lem:condDistrib_reward_stationaryEnv}

\end{proof}


\begin{lemma}\label{lem:condDistrib_rewardByCount_stepsUntil}
  \uses{def:stepsUntil, def:rewardByCount, def:Bandit.measure}
  \leanok
  \lean{Bandits.condDistrib_rewardByCount_stepsUntil}
For $n > 0$ and $t \in \mathbb{N}$, $P[Y_{n,a} \mid T_{n,a}] = \nu(a)$ (in which the measure on the r.h.s. is seen as a constant kernel).
\end{lemma}

\begin{proof}
It suffices to show that for all $t \in \mathbb{N} \cup \{\infty\}$ such that $\mathbb{P}(T_{n, a} = t) > 0$, the law of $Y_{n,a}$ conditioned on $T_{n,a} = t$ is $\nu(a)$.

If $t < \infty$, then $P[Y_{n, a} \mid T_{n, a} = t] = P[R_t \mid T_{n, a} = t] = \nu(a)$ by Lemma~\ref{lem:reward_cond_stepsUntil}.

If $t = \infty$, then $P[Y_{n, a} \mid T_{n, a} = \infty] = P[Z_{n, a} \mid T_{n, a} = \infty]$. By independence of $Z_{n,a}$ and $T_{n, a}$, this is just $\nu(a)$, the law of $Z_{n,a}$.
\end{proof}


\begin{lemma}\label{lem:hasLaw_rewardByCount}
  \uses{def:rewardByCount}
  \leanok
  \lean{Bandits.hasLaw_rewardByCount}
For $n > 0$ and $a \in \mathcal{A}$, the law of $Y_{n,a}$ is $\nu(a)$.
\end{lemma}

\begin{proof}
  \uses{lem:condDistrib_rewardByCount_stepsUntil}
It suffices to show that for all $t \in \mathbb{N} \cup \{\infty\}$, the law of $Y_{n,a}$ conditioned on $T_{n,a} = t$ is $\nu(a)$.
If $t = \infty$, then
\begin{align*}
  \mathcal{L}(Y_{n,a} \mid T_{n,a} = t)
  = \mathcal{L}(Z_{n,a} \mid T_{n,a} = t)
  = \mathcal{L}(Z_{n,a})
  = \nu(a)
\end{align*}
If $t < \infty$, then
\begin{align*}
  \mathcal{L}(Y_{n,a} \mid T_{n,a} = t)
  &= \mathcal{L}(R_t \mid T_{n,a} = t)
  \\
  &= \mathcal{L}(R_t \mid T_{n,a} = t, A_t = a)
  \\
  &= \mathcal{L}(R_t \mid A_t = a)
  \\
  &= \nu(a)
  \: .
\end{align*}
TODO: explain that chain of equalities. There is independence involved.
\end{proof}


\begin{lemma}\label{lem:iIndepFun_rewardByCount}
  \uses{def:rewardByCount}
  \leanok
  \lean{Bandits.iIndepFun_rewardByCount'}
The rewards $(Y_{n,a})_{n \in \mathbb{N}}$ are independent.
\end{lemma}

\begin{proof}
It suffices to show that for all $n \in \mathbb{N}$, $Y_{n+1, a}$ is independent of $(Y_{1,a}, \ldots, Y_{n,a})$.

TODO $H_{T_{n,a}}$, $H'_n$
\end{proof}


\begin{lemma}\label{lem:independent_rewardByCount}
  \uses{def:rewardByCount}
For $a \in \mathcal{A}$, let $Y^{(a)} = (Y_{n,a})_{n \in \mathbb{N}} \in \mathbb{R}^{\mathbb{N}}$ be the sequence of rewards obtained from pulling arm $a$. Then the sequences $(Y^{(a)})_{a \in \mathcal{A}}$ are independent.
\end{lemma}

\begin{proof}

\end{proof}


\begin{lemma}\label{lem:identDistrib_rewardByCount_stream}
  \uses{def:rewardByCount}
  \leanok
  \lean{Bandits.identDistrib_rewardByCount_stream}
The random sequences $(Y_{n+1,a})_{n \in \mathbb{N}}$ and $(Z_{n,a})_{n \in \mathbb{N}}$ are identically distributed.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:hasLaw_rewardByCount, lem:iIndepFun_rewardByCount}

\end{proof}


\begin{lemma}\label{lem:identDistrib_sum_Icc_rewardByCount}
  \uses{def:rewardByCount}
  \leanok
  \lean{Bandits.identDistrib_sum_Icc_rewardByCount}
The random variables $\sum_{i=1}^n Y_{i,a}$ and $\sum_{i=0}^{n-1} Z_{i,a}$ are identically distributed.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:identDistrib_rewardByCount_stream}
Immediate consequence of Lemma~\ref{lem:identDistrib_rewardByCount_stream}.
\end{proof}


\section{Regret and other bandit quantities}

\begin{definition}[Arm means]\label{def:armMean}
  \uses{def:bandit}
  \leanok % no actual Lean def, but we don't need one
For an arm $a \in \mathcal{A}$, we denote by $\mu_a$ the mean of the rewards for that arm, that is $\mu_a = \nu(a)[\mathrm{id}]$.
We denote by $\mu^*$ the mean of the best arm, that is $\mu^* = \max_{a \in \mathcal{A}} \mu_a$.
\end{definition}


\begin{definition}[Regret]\label{def:regret}
  \uses{def:armMean, def:actionReward}
  \leanok
  \lean{Bandits.regret}
The regret $R_T$ of a sequence of arms $A_0, \ldots, A_{T-1}$ after $T$ pulls is the difference between the cumulative reward of always playing the best arm and the cumulative reward of the sequence:
\begin{align*}
  R_T = T \mu^* - \sum_{t=0}^{T-1} \mu_{A_t} \: .
\end{align*}
\end{definition}


\begin{definition}\label{def:gap}
  \uses{def:armMean}
  \leanok
  \lean{Bandits.gap}
For an arm $a \in \mathcal{A}$, its gap is defined as the difference between the mean of the best arm and the mean of that arm: $\Delta_a = \mu^* - \mu_a$.
\end{definition}


\begin{lemma}\label{lem:sum_pullCount_mul}
  \uses{def:pullCount}
  \leanok
  \lean{Learning.sum_pullCount_mul}
Let $f : \mathcal{A} \to \mathbb{R}$ be a function on the arms. For all $t \in \mathbb{N}$,
\begin{align*}
  \sum_{a \in \mathcal{A}} N_{t,a} f(a) = \sum_{s=0}^{t-1} f(A_s) \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
\begin{align*}
  \sum_{a \in \mathcal{A}} N_{t,a} f(a)
  &= \sum_{a \in \mathcal{A}} \sum_{s=0}^{t-1} \mathbb{I}\{A_s = a\} f(a)
  \\
  &= \sum_{s=0}^{t-1} \sum_{a \in \mathcal{A}} \mathbb{I}\{A_s = a\} f(a)
  \\
  &= \sum_{s=0}^{t-1} f(A_s)
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:regret_eq_sum_pullCount_mul_gap}
  \uses{def:regret,def:gap,def:pullCount}
  \leanok
  \lean{Bandits.regret_eq_sum_pullCount_mul_gap}
For $\mathcal{A}$ finite, the regret $R_T$ can be expressed as a sum over the arms and their gaps:
\begin{align*}
  R_T = \sum_{a \in \mathcal{A}} N_{T,a} \Delta_a \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  \uses{lem:sum_pullCount_mul}
Apply Lemma~\ref{lem:sum_pullCount_mul} with $f(a) = \Delta_a$ to obtain:
\begin{align*}
  \sum_{a \in \mathcal{A}} N_{T,a} \Delta_a
  &= \sum_{s=0}^{T-1} \Delta_{A_s}
  \\
  &= \sum_{s=0}^{T-1} \mu^* - \sum_{s=0}^{T-1} \mu_{A_s}
  \\
  &= R_T
  \: .
\end{align*}
\end{proof}
