/-
Copyright (c) 2025 R√©my Degenne. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: R√©my Degenne, Paulo Rauber
-/
import LeanBandits.SequentialLearning.Algorithm

/-!
# Stationary environments
-/

open MeasureTheory ProbabilityTheory Filter Real Finset

open scoped ENNReal NNReal

namespace Learning

variable {Œ± R : Type*} {mŒ± : MeasurableSpace Œ±} {mR : MeasurableSpace R}

/-- A stationary environment, in which the distribution of the next reward depends only on the last
action. -/
@[simps]
def stationaryEnv (ŒΩ : Kernel Œ± R) [IsMarkovKernel ŒΩ] : Environment Œ± R where
  feedback _ := ŒΩ.prodMkLeft _
  ŒΩ0 := ŒΩ

variable {Œ© : Type*} {mŒ© : MeasurableSpace Œ©}
  [StandardBorelSpace Œ±] [Nonempty Œ±] [StandardBorelSpace R] [Nonempty R]
  {alg : Algorithm Œ± R} {ŒΩ : Kernel Œ± R} [IsMarkovKernel ŒΩ]
  {P : Measure Œ©} [IsProbabilityMeasure P] {A : ‚Ñï ‚Üí Œ© ‚Üí Œ±} {R' : ‚Ñï ‚Üí Œ© ‚Üí R}

section IsAlgEnvSeq

/-- The conditional distribution of the reward at time `n` given the action at time `n` is `ŒΩ`. -/
lemma IsAlgEnvSeq.condDistrib_reward_stationaryEnv
    (h : IsAlgEnvSeq A R' alg (stationaryEnv ŒΩ) P) (n : ‚Ñï) :
    condDistrib (R' n) (A n) P =·µê[P.map (A n)] ŒΩ := by
  have hA := h.measurable_A
  have hR' := h.measurable_R
  cases n with
  | zero =>
    rw [condDistrib_ae_eq_iff_measure_eq_compProd _ (by fun_prop)]
    change P.map (step A R' 0) = P.map (A 0) ‚äó‚Çò ŒΩ
    rw [(hasLaw_action_zero h).map_eq, (hasLaw_step_zero h).map_eq, stationaryEnv_ŒΩ0]
  | succ n =>
    have h_eq := (h.hasCondDistrib_reward n).condDistrib_eq
    rw [condDistrib_ae_eq_iff_measure_eq_compProd _ (by fun_prop)] at h_eq ‚ä¢
    have : P.map (A (n + 1)) =
        (P.map (fun x ‚Ü¶ (hist A R' n x, A (n + 1) x))).snd := by
      rw [Measure.snd_map_prodMk (by fun_prop)]
    simp only [stationaryEnv_feedback] at h_eq
    rw [this, ‚Üê Measure.snd_prodAssoc_compProd_prodMkLeft, ‚Üê h_eq,
      Measure.snd_map_prodMk (by fun_prop), Measure.map_map (by fun_prop) (by fun_prop)]
    congr

/-- The reward at time `n + 1` is conditionally independent of the history up to time `n`
given the action at time `n + 1`. -/
lemma IsAlgEnvSeq.condIndepFun_reward_hist_action [StandardBorelSpace Œ©] [Nonempty Œ©]
    (h : IsAlgEnvSeq A R' alg (stationaryEnv ŒΩ) P) (n : ‚Ñï) :
    R' (n + 1) ‚üÇ·µ¢[A (n + 1), h.measurable_A _ ; P] hist A R' n := by
  have hA := h.measurable_A
  have hR' := h.measurable_R
  exact condIndepFun_of_exists_condDistrib_prod_ae_eq_prodMkLeft
    (by fun_prop) (by fun_prop) (by fun_prop) (h.hasCondDistrib_reward n).condDistrib_eq

lemma IsAlgEnvSeq.condIndepFun_reward_hist_action_action [StandardBorelSpace Œ©] [Nonempty Œ©]
    (h : IsAlgEnvSeq A R' alg (stationaryEnv ŒΩ) P) (n : ‚Ñï) :
    R' (n + 1) ‚üÇ·µ¢[A (n + 1), h.measurable_A (n + 1); P]
      (fun œâ ‚Ü¶ (hist A R' n œâ, A (n + 1) œâ)) := by
  have h_indep : R' (n + 1) ‚üÇ·µ¢[A (n + 1), h.measurable_A (n + 1); P] hist A R' n := by
    convert h.condIndepFun_reward_hist_action n
  have hA := h.measurable_A
  have hR' := h.measurable_R
  exact h_indep.prod_right (by fun_prop) (by fun_prop) (by fun_prop)

lemma IsAlgEnvSeq.condIndepFun_reward_hist_action_action' [StandardBorelSpace Œ©] [Nonempty Œ©]
    (h : IsAlgEnvSeq A R' alg (stationaryEnv ŒΩ) P) (n : ‚Ñï) (hn : n ‚â† 0) :
    R' n ‚üÇ·µ¢[A n, h.measurable_A n; P] (fun œâ ‚Ü¶ (hist A R' (n - 1) œâ, A n œâ)) := by
  have := h.condIndepFun_reward_hist_action_action (n - 1)
  grind

end IsAlgEnvSeq

namespace IT

local notation "ùîì" => trajMeasure alg (stationaryEnv ŒΩ)

/-- The conditional distribution of the reward at time `n` given the action at time `n` is `ŒΩ`. -/
lemma condDistrib_reward_stationaryEnv (n : ‚Ñï) :
    condDistrib (IT.reward n) (IT.action n) ùîì =·µê[(ùîì).map (IT.action n)] ŒΩ := by
  cases n with
  | zero =>
    rw [condDistrib_ae_eq_iff_measure_eq_compProd _ (by fun_prop)]
    change (ùîì).map (IT.step 0) = (ùîì).map (IT.action 0) ‚äó‚Çò ŒΩ
    rw [(IT.hasLaw_action_zero alg (stationaryEnv ŒΩ)).map_eq,
      (IT.hasLaw_step_zero alg (stationaryEnv ŒΩ)).map_eq, stationaryEnv_ŒΩ0]
  | succ n =>
    have h_eq := IT.condDistrib_reward alg (stationaryEnv ŒΩ) n
    rw [condDistrib_ae_eq_iff_measure_eq_compProd _ (by fun_prop)] at h_eq ‚ä¢
    have : (ùîì).map (IT.action (n + 1)) =
        ((ùîì).map (fun x ‚Ü¶ (IT.hist n x, IT.action (n + 1) x))).snd := by
      rw [Measure.snd_map_prodMk (by fun_prop)]
    simp only [stationaryEnv_feedback] at h_eq
    rw [this, ‚Üê Measure.snd_prodAssoc_compProd_prodMkLeft, ‚Üê h_eq,
      Measure.snd_map_prodMk (by fun_prop), Measure.map_map (by fun_prop) (by fun_prop)]
    congr

/-- The reward at time `n + 1` is conditionally independent of the history up to time `n`
given the action at time `n + 1`. -/
lemma condIndepFun_reward_hist_action (n : ‚Ñï) :
    IT.reward (n + 1) ‚üÇ·µ¢[IT.action (n + 1), IT.measurable_action _ ; ùîì] IT.hist n :=
  condIndepFun_of_exists_condDistrib_prod_ae_eq_prodMkLeft
    (by fun_prop) (by fun_prop) (by fun_prop) (IT.condDistrib_reward alg (stationaryEnv ŒΩ) n)

lemma condIndepFun_reward_hist_action_action
    {alg : Algorithm Œ± R} {ŒΩ : Kernel Œ± R} [IsMarkovKernel ŒΩ] (n : ‚Ñï) :
    reward (n + 1) ‚üÇ·µ¢[action (n + 1), measurable_action (n + 1); trajMeasure alg (stationaryEnv ŒΩ)]
      (fun œâ ‚Ü¶ (hist n œâ, action (n + 1) œâ)) := by
  have h_indep : reward (n + 1) ‚üÇ·µ¢[action (n + 1), measurable_action (n + 1);
      trajMeasure alg (stationaryEnv ŒΩ)] hist n := by
    convert condIndepFun_reward_hist_action (alg := alg) (ŒΩ := ŒΩ) n
  exact h_indep.prod_right (by fun_prop) (by fun_prop) (by fun_prop)

lemma condIndepFun_reward_hist_action_action'
    {alg : Algorithm Œ± R} {ŒΩ : Kernel Œ± R} [IsMarkovKernel ŒΩ] (n : ‚Ñï) (hn : n ‚â† 0) :
    reward n ‚üÇ·µ¢[action n, measurable_action n; trajMeasure alg (stationaryEnv ŒΩ)]
      (fun œâ ‚Ü¶ (hist (n - 1) œâ, action n œâ)) := by
  have := condIndepFun_reward_hist_action_action (alg := alg) (ŒΩ := ŒΩ) (n - 1)
  grind

end IT

end Learning
