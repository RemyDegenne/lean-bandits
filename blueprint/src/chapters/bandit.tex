\chapter{Stochastic multi-armed bandits}

\section{Algorithm, bandit and probability space}

A bandit algorithm is an algorithm in the sense of Definition~\ref{def:algorithm}.
We call the actions \emph{arms} and the observations \emph{rewards}.

The first arm pulled by the algorithm is sampled from $P_0$, the arm pulled at time $1$ is sampled from $\pi_0(H_0)$, where $H_0 \in \mathcal{A} \times \mathcal{R}$ is the data of the first arm pulled and the first observation received, and so on.


\begin{definition}[Bandit]\label{def:bandit}
  \uses{def:stationaryEnv}
  \leanok
A stochastic bandit is simply a reward distribution for each arm: a Markov kernel $\nu : \mathcal{A} \rightsquigarrow \mathbb{R}$, conditional distribution of the rewards given the arm pulled.
It is a stationary environment in which the observation space is $\mathcal{R} = \mathbb{R}$.
\end{definition}

An algorithm can interact with a bandit to produce a sequence of arms and rewards: after a time $t$, the history $H_t \in (\mathcal{A} \times \mathcal{R})^{t+1}$ contains the arms pulled and rewards received up to that time,
\begin{itemize}
  \item the algorithm chooses an arm $A_{t+1}$ sampled according to its policy $\pi_t(H_t)$,
  \item the bandit generates a reward $R_{t+1}$ according to the distribution $\nu(A_{t+1})$,
  \item the history is updated to $H_{t+1} = ((A_0, R_0), \ldots, (A_{t+1}, R_{t+1}))$.
\end{itemize}

We now want to define a probability space on which we can study the sequences of arms and rewards, and formulate probabilistic statements about the interaction between the algorithm and the bandit.


\begin{definition}[Bandit probability space]\label{def:Bandit.measure}
  \uses{def:algorithm, def:bandit, def:trajMeasure}
  \leanok
  \lean{Bandits.Bandit.trajMeasure, Bandits.Bandit.measure}
As in Definition~\ref{def:trajMeasure}, an algorithm $(\pi, P_0)$ and bandit $\nu$ together defines a probability distribution $\mathbb{P}_{\mathcal{T}}$ on the space $\Omega_{\mathcal{T}} := (\mathcal{A} \times \mathbb{R})^{\mathbb{N}}$, the space of infinite sequences of arms and rewards.
We augment that probability space with a stream of rewards from each arm, independent of the bandit interaction, to get the probability space $(\Omega, \mathbb{P})$, where $\Omega = \Omega_{\mathcal{T}} \times \mathbb{R}^{\mathbb{N} \times \mathcal{A}}$ and $\mathbb{P} = \mathbb{P}_{\mathcal{T}} \otimes (\bigotimes_{n \in \mathbb{N}, a \in \mathcal{A}} \nu(a))$.
\end{definition}


\section{The array model of rewards}

We previously built a probability space on which we can define the sequence of arms and rewards generated by the interaction between the algorithm and the bandit, using the Ionescu-Tulcea theorem.
From Theorem~\ref{thm:isAlgEnvSeq_unique}, we know that the law of the sequence of arms and rewards is independent of the probability space used to define them.
Nonetheless, we now build an alternative model of the rewards, on which it will be easier to prove concentration inequalities.
By the uniqueness of the law, these statements will then transfer to any algorithm-environment interaction.

In the ``array model'', we consider a probability space on which we have an infinite array of rewards from each arm, independent from each other.
When pulling an arm, the algorithm sees the next previously unseen reward from that arm in the array.

\begin{definition}\label{def:arrayMeasure}
  \leanok
  \lean{Bandits.ArrayModel.probSpace, Bandits.ArrayModel.arrayMeasure}
Let $I = [0,1]$ and let $P_U$ be the uniform distribution on $I$. We define the probability space $(\Omega_{\mathcal{A}}, P_{\mathcal{A}})$, where
\begin{align*}
  \Omega_{\mathcal{A}} &:= I^{\mathbb{N}} \times \mathcal{R}^{\mathbb{N} \times \mathcal{A}}
  \: , \\
  P_{\mathcal{A}} &:= \left( \bigotimes_{n \in \mathbb{N}} P_U \right) \otimes \left( \bigotimes_{n \in \mathbb{N}, a \in \mathcal{A}} \nu(a) \right)
  \: .
\end{align*}
\end{definition}


\begin{definition}\label{def:algFunction}
  \uses{def:algorithm}
  \leanok
  \lean{Bandits.ArrayModel.algFunction, Bandits.ArrayModel.initAlgFunction}
Let $\mathfrak{A}$ be an algorithm with action space $\mathcal{A}$ and reward space $\mathcal{R}$, policy $\pi$ and initial distribution $P_0$.
For $\mathcal{A}$ and $\mathcal{R}$ standard Borel spaces, there exists jointly measurable functions $f'_0 : I \to \mathcal{A}$ and $f_t : (\mathcal{A} \times \mathcal{R})^{t+1} \times I \to \mathcal{A}$ such that
\begin{itemize}
  \item the law of $f'_0$ is $P_0$,
  \item for all history $h_t \in (\mathcal{A} \times \mathcal{R})^{t+1}$, the law of $f_t(h_t, \cdot)$ is $\pi_t(h_t)$.
\end{itemize}
\end{definition}


\begin{definition}\label{def:AM.history}
  \uses{def:algFunction, def:arrayMeasure, def:pullCount}
  \leanok
  \lean{Bandits.ArrayModel.hist, Bandits.ArrayModel.action, Bandits.ArrayModel.reward}
The history, actions and rewards on the array model probability space $(\Omega_{\mathcal{A}}, P_{\mathcal{A}})$ are defined as follows:
\begin{itemize}
  \item the action at time $0$ is $A_0(\omega) = f'_0(\omega_{1,0})$, the reward at time $0$ is $R_0(\omega) = \omega_{2,0,A_0(\omega)}$, and the history at time $0$ is $H_0(\omega) = (A_0(\omega), R_0(\omega))$,
  \item for $t \ge 0$, the action at time $t+1$ is $A_{t+1}(\omega) = f_t(H_t(\omega), \omega_{1,t+1})$, the reward at time $t+1$ is $R_{t+1}(\omega) = \omega_{2,N_{t+1,A_{t+1}(\omega)},A_{t+1}(\omega)}$, and the history at time $t+1$ is $H_{t+1}(\omega) = (H_t(\omega), (A_{t+1}(\omega), R_{t+1}(\omega)))$.
\end{itemize}
\end{definition}


The goal of this section is to show that $(\Omega_{\mathcal{A}}, P_{\mathcal{A}})$ with the actions and rewards defined above is an algorithm-environment sequence as in Definition~\ref{def:IsAlgEnvSeq}.


\subsection{Measurability}

TODO: some of those results are proved for $\mathcal{A}$ countable. Add that assumption where needed.

\begin{remark}[Proving measurability with respect to a sub-sigma-algebra]
In this section, we often need to prove that a random variable $X$ is measurable with respect to the sigma-algebra generated by a subset of the independent random variables defining the probability space $\Omega_{\mathcal{A}}$.
We want to prove that $X : \Omega_{\mathcal{A}} \to \mathcal{X}$ is measurable with respect to the sigma-algebra $\sigma((\omega_p)_{p \in S})$, where $S$ is a subset of the indices of the independent random variables defining $\Omega_{\mathcal{A}}$.
In most cases, this is due to $X$ being defined (possibly recursively in a complicated way) only using those random variables.
However, it might be difficult to exhibit an explicit function $f$ such that $X = f((\omega_p)_{p \in S})$.

Here is a general strategy to prove such measurability results in Lean:
\begin{enumerate}
  \item Prove that $X$ is measurable with respect to the full sigma-algebra on $\Omega_{\mathcal{A}}$.
  \item Prove a congruence lemma: for any $\omega, \omega' \in \Omega_{\mathcal{A}}$, if $\omega_p = \omega'_p$ for all $p \in S$, then $X(\omega) = X(\omega')$.
  \item Define $g : (\prod_{p \in S} \Omega_p) \to \Omega$ by $g((\omega_p)_{p \in S}) = \omega'$, where $\omega'_p = \omega_p$ for $p \in S$ and $\omega'_p$ is some fixed value for $p \notin S$.
  \item Write $X = X \circ g \circ \mathrm{proj}_S$, where $\mathrm{proj}_S : \Omega_{\mathcal{A}} \to \prod_{p \in S} \Omega_p$ is the projection on the coordinates in $S$.
  \item Conclude that $X$ is the composition of a measurable function $(X \circ g)$ and the random variable generating the sub-sigma-algebra, and thus is measurable with respect to that sub-sigma-algebra.
\end{enumerate}
\end{remark}

\begin{lemma}[Measurability]\label{lem:AM.measurable_hist}
  \uses{def:AM.history}
  \leanok
  \lean{Bandits.ArrayModel.measurable_hist, Bandits.ArrayModel.measurable_action, Bandits.ArrayModel.measurable_reward}
$H_t$, $N_{t,A_t}$, $A_t$ and $R_t$ are measurable for all $t \in \mathbb{N}$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}[Congruence for the history]\label{lem:AM.hist_congr}
  \uses{def:AM.history}
  \leanok
  \lean{Bandits.ArrayModel.hist_congr}
Let $\omega, \omega' \in \Omega_{\mathcal{A}}$ and $t \in \mathbb{N}$.
Suppose that
\begin{align*}
  \forall s \le t, \ \omega_{1,s} &= \omega'_{1,s}
  \: , \\
  \forall a \in \mathcal{A}, \forall s < N_{t+1,a}, \ \omega_{2,s,a} &= \omega'_{2,s,a}
  \: .
\end{align*}
Then $H_t(\omega) = H_t(\omega')$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}[Congruence for the number of pulls and action]\label{lem:AM.stepsUntil_congr}
  \uses{def:AM.history, def:pullCount}
  \leanok
  \lean{Bandits.ArrayModel.stepsUntil_congr}
Let $\omega, \omega' \in \Omega_{\mathcal{A}}$, $t, m \in \mathbb{N}$ and $a \in \mathcal{A}$.
Suppose that
\begin{align*}
  \omega_{1} &= \omega'_{1}
  \: , \\
  \forall s < m, \ \omega_{2,s,a} &= \omega'_{2,s,a}
  \: , \\
  \forall b \ne a, \forall s \in \mathbb{N}, \ \omega_{2,s,b} &= \omega'_{2,s,b}
  \: .
\end{align*}
Then $(N_{t+1, a}(\omega) = m \wedge A_{t+1}(\omega) = a) \iff (N_{t+1, a}(\omega') = m \wedge A_{t+1}(\omega') = a)$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.hist_congr}

\end{proof}


\begin{definition}\label{def:AM.probSpaceSubsets}
  \leanok
  \lean{Bandits.ArrayModel.truePast}
We define the following functions on $\Omega_{\mathcal{A}}$:
\begin{align*}
  F_{1, t}(\omega) &= ((\omega_{1,s})_{s \le t}, (\omega_{2,s})_{s \in \mathbb{N}})
  \: , \\
  F_{2, a, t}(\omega) &= ((\omega_{1,s})_{s \in \mathbb{N}}, (\omega_{2, \min\{s,N_{t+1,a}(\omega)-1\}, a})_{s \in \mathbb{N}}, (\omega_{2, s, b})_{s \in \mathbb{N}, b \ne a})
  \: , \\
  F_{2, a}^m(\omega) &= ((\omega_{1,s})_{s \in \mathbb{N}}, (\omega_{2, \min\{s,m-1\}, a})_{s \in \mathbb{N}}, (\omega_{2, s, b})_{s \in \mathbb{N}, b \ne a})
  \: .
\end{align*}
In the definition of $F_{2, a, t}$ and $F_{2, a}^m$, if $N_{t+1,a}(\omega) = 0$ (resp. $m = 0$), then the second component is a constant sequence equal to an arbitrary value.

$F_{1, t}(\omega)$ contains all the information in $\omega$ except for the action selection randomness after time $t$.

$F_{2, a, t}(\omega)$ contains all the information in $\omega$ except the rewards for arm $a$ indexed by $N_{t+1,a}(\omega)$ or more.
$F_{2, a}^m(\omega)$ is similar, but removes the rewards for arm $a$ indexed by $m$ or more.
\end{definition}


\begin{lemma}\label{lem:AM.measurable_hist_todo}
  \uses{def:AM.probSpaceSubsets, def:AM.history}
  \leanok
  \lean{Bandits.ArrayModel.measurable_hist_todo, Bandits.ArrayModel.measurable_hist_truePast}
For all $t \in \mathbb{N}$, $H_t$ is measurable with respect to the sigma-algebra generated by $F_{1, t}$, and with respect to the sigma-algebra generated by $F_{2, a, t}$ for any arm $a \in \mathcal{A}$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.measurable_hist, lem:AM.hist_congr}

\end{proof}


\begin{lemma}\label{lem:AM.measurable_action_add_one_truePast}
  \uses{def:AM.probSpaceSubsets, def:AM.history}
  \leanok
  \lean{Bandits.ArrayModel.measurable_action_add_one_truePast}
$A_{t+1}$ is measurable with respect to the sigma-algebra generated by $F_{2, a, t}$ for any arm $a \in \mathcal{A}$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.measurable_hist_todo}

\end{proof}


\begin{lemma}\label{lem:AM.measurable_pullCount_add_one_truePast}
  \uses{def:AM.probSpaceSubsets, def:pullCount}
  \leanok
  \lean{Bandits.ArrayModel.measurable_pullCount_add_one_truePast}
$N_{t+1,a}$ is measurable with respect to the sigma-algebra generated by $F_{2, a, t}$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.measurable_hist_todo}

\end{proof}


\begin{lemma}\label{lem:AM.measurable_stepsUntil}
  \uses{def:AM.probSpaceSubsets, def:pullCount}
  \leanok
  \lean{Bandits.ArrayModel.measurable_stepsUntil}
For $t, m \in \mathbb{N}$ and $a \in \mathcal{A}$, the indicator function $\mathbb{I}\{N_{t+1,a} = m \wedge A_{t+1} = a\} : \Omega_{\mathcal{A}} \to \{0, 1\}$ is measurable with respect to the sigma-algebra generated by $F_{2, a}^m$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.stepsUntil_congr, lem:AM.measurable_hist}

\end{proof}


\begin{lemma}\label{lem:AM.measurable_pullCount_action_add_one_hist}
  \uses{def:AM.history, def:pullCount}
  \leanok
  \lean{Bandits.ArrayModel.measurable_pullCount_action_add_one_hist}
For $t \in \mathbb{N}$, the function $N_{t+1, A_{t+1}}$ is measurable with respect to the sigma-algebra generated by $H_t$ and $A_{t+1}$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:pullCount_basic}

\end{proof}


\subsection{Independence}


\begin{lemma}\label{lem:AM.indepFun_fst_add_one_aux}
  \uses{def:AM.probSpaceSubsets}
  \leanok
  \lean{Bandits.ArrayModel.indepFun_fst_add_one_aux}
$\omega \mapsto \omega_{1, t+1}$ is independent of $F_{1, t}$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:AM.indepFun_fst_add_one_hist}
  \uses{def:AM.history, def:AM.probSpaceSubsets}
  \leanok
  \lean{Bandits.ArrayModel.indepFun_fst_add_one_hist}
$\omega \mapsto \omega_{1, t+1}$ is independent of $H_t$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.measurable_hist_todo, lem:AM.indepFun_fst_add_one_aux}

\end{proof}


\begin{lemma}\label{lem:AM.indepFun_snd_apply_aux}
  \uses{def:AM.probSpaceSubsets}
  \leanok
  \lean{Bandits.ArrayModel.indepFun_snd_apply_aux}
For $a \in \mathcal{A}$ and $m \in \mathbb{N}$, $\omega \mapsto \omega_{2, m, a}$ is independent of $F_{2, a}^m$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:AM.indepFun_snd_apply_pullCount_action}
  \uses{def:AM.history, def:pullCount, def:AM.probSpaceSubsets}
  \leanok
  \lean{Bandits.ArrayModel.indepFun_snd_apply_pullCount_action}
For $a \in \mathcal{A}$, $\omega \mapsto \omega_{2, m, a}$ is independent of the indicator function $\mathbb{I}\{N_{t+1,a} = m \wedge A_{t+1} = a\}$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.measurable_stepsUntil, lem:AM.indepFun_snd_apply_aux}

\end{proof}


\begin{lemma}\label{lem:AM.indepFun_snd_hist_cond}
  \uses{def:AM.history, def:pullCount}
  \leanok
  \lean{Bandits.ArrayModel.indepFun_snd_hist_cond}
For $a \in \mathcal{A}$ and $t, m \in \mathbb{N}$, $\omega \mapsto \omega_{2, m, a}$ is independent of $H_t$ given that $N_{t+1,a} = m$ and $A_{t+1} = a$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.measurable_stepsUntil, lem:AM.measurable_hist_todo, lem:AM.indepFun_snd_apply_aux}

\end{proof}


\subsection{Laws}


\begin{lemma}\label{lem:AM.hasLaw_action_zero}
  \uses{def:AM.history, def:algFunction}
  \leanok
  \lean{Bandits.ArrayModel.hasLaw_action_zero}
The law of $A_0$ in the array model is $P_0$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:AM.hasCondDistrib_reward_zero}
  \uses{def:AM.history}
  \leanok
  \lean{Bandits.ArrayModel.hasCondDistrib_reward_zero}
In the array model, $P_{\mathcal{A}}[R_0 \mid A_0] = \nu$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:AM.hasCondDistrib_action}
  \uses{def:AM.history}
  \leanok
  \lean{Bandits.ArrayModel.hasCondDistrib_action}
In the array model, $P_{\mathcal{A}}[A_{t+1} \mid H_t] = \pi_t$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.indepFun_fst_add_one_hist, def:algFunction}

\end{proof}


\begin{lemma}\label{lem:AM.hasCondDistrib_reward_pullCount_action}
  \uses{def:AM.history, def:pullCount}
  \leanok
  \lean{Bandits.ArrayModel.hasCondDistrib_reward_pullCount_action}
In the array model, $P_{\mathcal{A}}[R_{t+1} \mid N_{t+1,A_{t+1}}, A_{t+1}] = \nu$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.indepFun_snd_apply_pullCount_action}

\end{proof}


\begin{lemma}\label{lem:AM.hasCondDistrib_reward_hist_action_pullCount}
  \uses{def:AM.history, def:pullCount}
  \leanok
  \lean{Bandits.ArrayModel.hasCondDistrib_reward_hist_action_pullCount}
In the array model, $P_{\mathcal{A}}[R_{t+1} \mid H_t, A_{t+1}, N_{t+1,A_{t+1}}] = \nu$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.indepFun_snd_hist_cond, lem:AM.indepFun_snd_apply_pullCount_action}

\end{proof}


\begin{lemma}\label{lem:AM.condIndepFun_reward_hist}
  \uses{def:AM.history}
  \leanok
  \lean{Bandits.ArrayModel.condIndepFun_reward_hist}
For $t \ge 0$, $R_{t+1} \ind H_t \mid A_{t+1}, N_{t+1, A_{t+1}}$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.hasCondDistrib_reward_hist_action_pullCount}

\end{proof}


\begin{lemma}\label{lem:AM.hasCondDistrib_reward}
  \uses{def:AM.history}
  \leanok
  \lean{Bandits.ArrayModel.hasCondDistrib_reward}
In the array model, $P_{\mathcal{A}}[R_{t+1} \mid H_t, A_{t+1}] = \nu$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:AM.measurable_pullCount_action_add_one_hist, lem:AM.condIndepFun_reward_hist,
  lem:AM.hasCondDistrib_reward_pullCount_action}

\end{proof}

\begin{theorem}\label{thm:isAlgEnvSeq_arrayMeasure}
  \uses{def:bandit, def:arrayMeasure, def:IsAlgEnvSeq}
  \leanok
  \lean{Bandits.ArrayModel.isAlgEnvSeq_arrayMeasure}
The actions and rewards defined on the array model probability space $(\Omega_{\mathcal{A}}, P_{\mathcal{A}})$ form an algorithm-environment sequence for the algorithm $\mathfrak{A}$ and bandit $\nu$.
\end{theorem}

\begin{proof}\leanok
  \uses{lem:AM.hasLaw_action_zero, lem:AM.hasCondDistrib_reward_zero,
  lem:AM.hasCondDistrib_action, lem:AM.hasCondDistrib_reward}
The four conditions of Definition~\ref{def:IsAlgEnvSeq} are satisfied by Lemmas~\ref{lem:AM.hasLaw_action_zero}, \ref{lem:AM.hasCondDistrib_reward_zero}, \ref{lem:AM.hasCondDistrib_action} and \ref{lem:AM.hasCondDistrib_reward}.
\end{proof}




\section{Law of the n\textsuperscript{th} pull}\label{sec:alt_model}

This section describes the law of $Y_{n, a}$ the $n^{th}$ reward obtained from an arm $a \in \mathcal{A}$ (Definition~\ref{def:rewardByCount}).

We augment the probability space $\Omega$ on which we have an algorithm-environment sequence with $\Omega' = \mathbb{R}^{\mathbb{N} \times \mathcal{A}}$, on which we put the product measure $\bigotimes_{n \in \mathbb{N}, a \in \mathcal{A}} \nu(a)$.
With that measure, the law of $Z_{n,a}$ is $\nu(a)$.


\begin{lemma}\label{lem:measurable_comap_indicator_stepsUntil_eq}
  \uses{def:stepsUntil}
  \leanok
  \lean{Learning.measurable_comap_indicator_stepsUntil_eq}
The function $\mathbb{I}\{T_{n,a} = t\} : \Omega \to \{0, 1\}$ is measurable with respect to the sigma-algebra generated by $(H_{t-1}, A_t)$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:condIndepFun_reward_stepsUntil_arm}
  \uses{def:stepsUntil, def:IT.actionReward, def:Bandit.measure}
  \leanok
  \lean{Bandits.condIndepFun_reward_stepsUntil_action}
For $t > 0$, $R_t \ind \mathbb{I}\{T_{n, a} = t\} \mid A_t$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:measurable_comap_indicator_stepsUntil_eq, lem:condIndepFun_reward_hist_action, lem:CondIndepFun.prod_right}
$\mathbb{I}\{T_{n, a} = t\}$ is measurable with respect to the sigma-algebra generated by $(H_{t-1}, A_t)$ by Lemma~\ref{lem:measurable_comap_indicator_stepsUntil_eq}.

It thus suffices to show that $R_t \ind (H_{t-1}, A_t) \mid A_t$, which is implied by $R_t \ind H_{t-1} \mid A_t$ (Lemma~\ref{lem:CondIndepFun.prod_right}), which is Lemma~\ref{lem:condIndepFun_reward_hist_action}.
\end{proof}


\begin{lemma}\label{lem:reward_cond_stepsUntil}
  \uses{def:stepsUntil, def:rewardByCount, def:Bandit.measure}
  \leanok
  \lean{Bandits.reward_cond_stepsUntil}
Let $n > 0$, $t \in \mathbb{N}$ and suppose that $P(T_{n, a} = t) > 0$.
Then $\mathcal{L}(R_t \mid T_{n, a} = t) = \nu(a)$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:stepsUntil_basic, lem:condIndepFun_reward_stepsUntil_arm, lem:condDistrib_reward_stationaryEnv}
First, if $T_{n, a} = t$, then $A_t = a$ (Lemma~\ref{lem:stepsUntil_basic}), such that $\mathcal{L}(R_t \mid T_{n, a} = t) = \mathcal{L}(R_t \mid T_{n, a} = t, A_t = a)$.

Then, using first the independence from Lemma~\ref{lem:condIndepFun_reward_stepsUntil_arm} and then the conditional distribution from Lemma~\ref{lem:condDistrib_reward_stationaryEnv}, we have
\begin{align*}
  \mathcal{L}(R_t \mid T_{n, a} = t, A_t = a)
  &= \mathcal{L}(R_t \mid A_t = a)
  = \nu(a)
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:condDistrib_ae_eq_cond}
  \leanok
  \lean{ProbabilityTheory.condDistrib_ae_eq_cond}
For a random variable $X$ on a countable space with the discrete sigma algebra, $\mathcal{L}(Y \mid X) = (x \mapsto \mathcal{L}(Y \mid X = x))$, $(X_*P)$-almost surely.
Furthermore, that almost sure equality means that for all $x$ such that $P(X = x) > 0$, we have $\mathcal{L}(Y \mid X = x) = \mathcal{L}(Y \mid X)(x)$.
\end{lemma}

\begin{proof}\leanok

\end{proof}


\begin{lemma}\label{lem:condDistrib_rewardByCount_stepsUntil}
  \uses{def:stepsUntil, def:rewardByCount, def:Bandit.measure}
  \leanok
  \lean{Bandits.condDistrib_rewardByCount_stepsUntil}
For $n > 0$ and $t \in \mathbb{N}$, $\mathcal{L}(Y_{n,a} \mid T_{n,a}) = \nu(a)$ (in which the measure on the r.h.s. is seen as a constant kernel).
\end{lemma}

\begin{proof}\leanok
  \uses{lem:condDistrib_ae_eq_cond, lem:reward_cond_stepsUntil}
It suffices to show that for all $t \in \mathbb{N} \cup \{\infty\}$ such that $\mathbb{P}(T_{n, a} = t) > 0$, the law of $Y_{n,a}$ conditioned on $T_{n,a} = t$ is $\nu(a)$.

If $t < \infty$, then $\mathcal{L}(Y_{n, a} \mid T_{n, a} = t) = \mathcal{L}(R_t \mid T_{n, a} = t) = \nu(a)$ by Lemma~\ref{lem:reward_cond_stepsUntil}.

If $t = \infty$, then $\mathcal{L}(Y_{n, a} \mid T_{n, a} = \infty) = \mathcal{L}(Z_{n, a} \mid T_{n, a} = \infty)$. By independence of $Z_{n,a}$ and $T_{n, a}$, this is just $\nu(a)$, the law of $Z_{n,a}$.
\end{proof}


\begin{lemma}\label{lem:hasLaw_rewardByCount}
  \uses{def:rewardByCount}
  \leanok
  \lean{Bandits.hasLaw_rewardByCount}
For $n > 0$ and $a \in \mathcal{A}$, $\mathcal{L}(Y_{n,a}) = \nu(a)$.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:condDistrib_rewardByCount_stepsUntil}
The law of $Y_{n,a}$ is given by $\mathcal{L}(Y_{n, a}) = \mathcal{L}(Y_{n, a} \mid T_{n, a}) \circ \mathcal{L}(T_{n, a})$.
By Lemma~\ref{lem:condDistrib_rewardByCount_stepsUntil}, $\mathcal{L}(Y_{n, a} \mid T_{n, a}) = \nu(a)$, a constant kernel.
Thus the composition is just $\nu(a)$.
\end{proof}


% \begin{lemma}\label{lem:indepFun_rewardByCount_stepsUntil}
%   \uses{def:stepsUntil, def:rewardByCount, def:Bandit.measure}
% For $n > 0$, $Y_{n, a} \ind T_{n,a}$.
% \end{lemma}

% \begin{proof}
%   \uses{lem:condDistrib_rewardByCount_stepsUntil, lem:hasLaw_rewardByCount}
% It suffices to prove that $\mathcal{L}(Y_{n,a} \mid T_{n,a}) = \mathcal{L}(Y_{n,a})$.

% By Lemma~\ref{lem:hasLaw_rewardByCount}, $\mathcal{L}(Y_{n,a}) = \nu(a)$.
% By Lemma~\ref{lem:condDistrib_rewardByCount_stepsUntil}, $\mathcal{L}(Y_{n,a} \mid T_{n,a}) = \nu(a)$.
% \end{proof}


% \begin{lemma}\label{lem:condIndepFun_rewardByCount_hist}
%   \uses{def:rewardByCount, def:stepsUntil, def:Bandit.measure}
% For $n > 0$, $Y_{n, a} \ind H_{T_{n,a}-1} \mid T_{n,a}$, in which $H_{\infty}$ is interpreted as the whole history.
% \end{lemma}

% \begin{proof}
%   \uses{lem:condDistrib_rewardByCount_stepsUntil, lem:condIndepFun_reward_hist_action}
% By Lemma~\ref{lem:condDistrib_rewardByCount_stepsUntil}, $\mathcal{L}(Y_{n,a} \mid T_{n,a}) = \nu(a)$.
% We need to prove that $\mathcal{L}(Y_{n,a} \mid H_{T_{n,a}-1}, T_{n,a}) = \nu(a)$.
% By Lemma~\ref{lem:stepsUntil_basic}, if $T_{n,a} = t \in \mathbb{N}$, then $A_t = a$.
% We get
% \begin{align*}
%   \mathcal{L}(Y_{n,a} \mid H_{T_{n,a}-1}, T_{n,a} = t)
%   &= \mathcal{L}(R_t \mid H_{t-1}, T_{n,a} = t, A_t = a)
%   \: .
% \end{align*}
% Then, $\mathbb{I}\{T_{n,a} = t\}$ is a function of $(H_{t-1}, A_t)$ by Lemma~\ref{lem:measurable_comap_indicator_stepsUntil_eq}, such that

% \begin{align*}
%   \mathcal{L}(R_t \mid H_{t-1}, T_{n,a} = t, A_t = a)
%   &= \mathcal{L}(R_t \mid H_{t-1}, A_t = a)
%   \: .
% \end{align*}
% Thus, using Lemma~\ref{lem:condIndepFun_reward_hist_action}, we have
% \begin{align*}
%   \mathcal{L}(R_t \mid H_{t-1}, A_t = a)
%   = \nu(a)
%   \: .
% \end{align*}

% If $T_{n,a} = \infty$, then $\mathcal{L}(Y_{n,a} \mid H_{T_{n,a}-1}, T_{n,a} = \infty) = \mathcal{L}(Z_{n,a} \mid H_{\infty}, T_{n,a} = \infty) = \nu(a)$ by independence of $Z_{n,a}$ from the history and $T_{n,a}$.

% \end{proof}


% \begin{lemma}\label{lem:indepFun_rewardByCount_hist_stepsUntil}
%   \uses{def:rewardByCount, def:stepsUntil, def:Bandit.measure}
% For $n > 0$, $Y_{n, a} \ind (H_{T_{n,a}-1}, T_{n,a})$.
% \end{lemma}

% \begin{proof}
%   \uses{lem:condIndepFun_rewardByCount_hist}
% By Lemma~\ref{lem:condIndepFun_rewardByCount_hist}, $Y_{n, a} \ind H_{T_{n,a}-1} \mid T_{n,a}$.
% By Lemma~\ref{lem:indepFun_rewardByCount_stepsUntil}, $Y_{n, a} \ind T_{n,a}$.
% By the contraction property of conditional independence (Lemma~\ref{lem:indepFun_contraction}), we have $Y_{n, a} \ind (H_{T_{n,a}-1}, T_{n,a})$.
% \end{proof}


% \begin{lemma}\label{lem:iIndepFun_rewardByCount}
%   \uses{def:rewardByCount}
% The rewards $(Y_{n,a})_{n \in \mathbb{N}}$ are independent.
% \end{lemma}

% \begin{proof}
%   \uses{lem:iIndepFun_nat_iff_forall_indepFun, lem:indepFun_contraction, lem:indepFun_rewardByCount_stepsUntil}
% By Lemma~\ref{lem:iIndepFun_nat_iff_forall_indepFun}, it suffices to show that for all $n \in \mathbb{N}$, $Y_{n+1, a}$ is independent of $(Y_{1,a}, \ldots, Y_{n,a})$.

% By the contraction property of conditional independence (Lemma~\ref{lem:indepFun_contraction}), it suffices to show that $Y_{n+1, a}$ is independent of $(Y_{1,a}, \ldots, Y_{n,a})$ conditionally on $T_{n+1, a}$ and that $Y_{n+1, a}$ is independent of $T_{n+1, a}$.

% The fact that $Y_{n+1, a}$ is independent of $T_{n+1, a}$ is Lemma~\ref{lem:indepFun_rewardByCount_stepsUntil}.

% TODO

% \end{proof}


% \begin{lemma}\label{lem:independent_rewardByCount}
%   \uses{def:rewardByCount}
% For $a \in \mathcal{A}$, let $Y^{(a)} = (Y_{n,a})_{n \in \mathbb{N}} \in \mathbb{R}^{\mathbb{N}}$ be the sequence of rewards obtained from pulling arm $a$. Then the sequences $(Y^{(a)})_{a \in \mathcal{A}}$ are independent.
% \end{lemma}

% \begin{proof}

% \end{proof}


% \begin{lemma}\label{lem:identDistrib_rewardByCount_stream}
%   \uses{def:rewardByCount}
% The random sequences $(Y_{n+1,a})_{n \in \mathbb{N}}$ and $(Z_{n,a})_{n \in \mathbb{N}}$ are identically distributed.
% \end{lemma}

% \begin{proof}
%   \uses{lem:hasLaw_rewardByCount, lem:iIndepFun_rewardByCount}

% \end{proof}


% \begin{lemma}\label{lem:identDistrib_sum_Icc_rewardByCount}
%   \uses{def:rewardByCount}
% The random variables $\sum_{i=1}^n Y_{i,a}$ and $\sum_{i=0}^{n-1} Z_{i,a}$ are identically distributed.
% \end{lemma}

% \begin{proof}
%   \uses{lem:identDistrib_rewardByCount_stream}
% Immediate consequence of Lemma~\ref{lem:identDistrib_rewardByCount_stream}.
% \end{proof}


\section{Regret and other bandit quantities}

\begin{definition}[Arm means]\label{def:armMean}
  \uses{def:bandit}
  \leanok % no actual Lean def, but we don't need one
For an arm $a \in \mathcal{A}$, we denote by $\mu_a$ the mean of the rewards for that arm, that is $\mu_a = \nu(a)[\mathrm{id}]$.
We denote by $\mu^*$ the mean of the best arm, that is $\mu^* = \max_{a \in \mathcal{A}} \mu_a$.
\end{definition}


\begin{definition}[Regret]\label{def:regret}
  \uses{def:armMean, def:IT.actionReward}
  \leanok
  \lean{Bandits.regret}
The regret $R_T$ of a sequence of arms $A_0, \ldots, A_{T-1}$ after $T$ pulls is the difference between the cumulative reward of always playing the best arm and the cumulative reward of the sequence:
\begin{align*}
  R_T = T \mu^* - \sum_{t=0}^{T-1} \mu_{A_t} \: .
\end{align*}
\end{definition}


\begin{definition}\label{def:gap}
  \uses{def:armMean}
  \leanok
  \lean{Bandits.gap}
For an arm $a \in \mathcal{A}$, its gap is defined as the difference between the mean of the best arm and the mean of that arm: $\Delta_a = \mu^* - \mu_a$.
\end{definition}


\begin{lemma}\label{lem:sum_pullCount_mul}
  \uses{def:pullCount}
  \leanok
  \lean{Learning.sum_pullCount_mul}
Let $f : \mathcal{A} \to \mathbb{R}$ be a function on the arms. For all $t \in \mathbb{N}$,
\begin{align*}
  \sum_{a \in \mathcal{A}} N_{t,a} f(a) = \sum_{s=0}^{t-1} f(A_s) \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
\begin{align*}
  \sum_{a \in \mathcal{A}} N_{t,a} f(a)
  &= \sum_{a \in \mathcal{A}} \sum_{s=0}^{t-1} \mathbb{I}\{A_s = a\} f(a)
  \\
  &= \sum_{s=0}^{t-1} \sum_{a \in \mathcal{A}} \mathbb{I}\{A_s = a\} f(a)
  \\
  &= \sum_{s=0}^{t-1} f(A_s)
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:regret_eq_sum_pullCount_mul_gap}
  \uses{def:regret,def:gap,def:pullCount}
  \leanok
  \lean{Bandits.regret_eq_sum_pullCount_mul_gap}
For $\mathcal{A}$ finite, the regret $R_T$ can be expressed as a sum over the arms and their gaps:
\begin{align*}
  R_T = \sum_{a \in \mathcal{A}} N_{T,a} \Delta_a \: .
\end{align*}
\end{lemma}

\begin{proof}\leanok
  \uses{lem:sum_pullCount_mul}
Apply Lemma~\ref{lem:sum_pullCount_mul} with $f(a) = \Delta_a$ to obtain:
\begin{align*}
  \sum_{a \in \mathcal{A}} N_{T,a} \Delta_a
  &= \sum_{s=0}^{T-1} \Delta_{A_s}
  \\
  &= \sum_{s=0}^{T-1} \mu^* - \sum_{s=0}^{T-1} \mu_{A_s}
  \\
  &= R_T
  \: .
\end{align*}
\end{proof}
