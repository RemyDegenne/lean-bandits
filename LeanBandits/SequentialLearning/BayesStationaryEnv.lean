/-
Copyright (c) 2026 Rémy Degenne. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: Rémy Degenne, Paulo Rauber
-/
import LeanBandits.SequentialLearning.IonescuTulceaSpace

open MeasureTheory ProbabilityTheory Finset

namespace Learning.Bayes

variable {α R E : Type*} [mα : MeasurableSpace α] [mR : MeasurableSpace R] [mE : MeasurableSpace E]
variable (Q : Measure E) [IsProbabilityMeasure Q] (κ : Kernel (α × E) R) [IsMarkovKernel κ]

/-- Given a prior distribution `Q` over "environments" and a kernel `k` that defines a reward
distribution `κ (a, e)` for each action `a : α` and "environment" `e : E`, a StationaryEnv
represents an environment (with an observation space `E × R`) that draws an environment `e : E` at
the very first step which, together with `k`, defines how the bandit process behaves. Because the
"environment" `e` is repeated at every step and reveals the best arm, it only makes sense to study
algorithms that ignore it, which is why `trajMeasure` is created from an algorithm whose
observation space is just `R` (rather than `E × R`). -/
noncomputable
def StationaryEnv : Environment α (E × R) where
  feedback n :=
    let g : (Iic n → α × E × R) × α → α × E := fun (h, a) => (a, (h ⟨0, by simp⟩).2.1)
    (Kernel.deterministic (Prod.snd ∘ g) (by fun_prop)) ×ₖ (κ.comap g (by fun_prop))
  ν0 := (Kernel.const α Q) ⊗ₖ κ

/-- Measure on the sequence of actions and observations (in `E × R`) generated by the
algorithm/environment. -/
noncomputable
def trajMeasure (alg : Algorithm α R) :=
  Learning.trajMeasure (alg.prod_left E) (StationaryEnv Q κ)
deriving IsProbabilityMeasure

/-- `action n` is the action pulled at time `n`. -/
def action (n : ℕ) (ω : ℕ → α × E × R) : α := (ω n).1

/-- `reward n` is the reward at time `n`. -/
def reward (n : ℕ) (ω : ℕ → α × E × R) : R := (ω n).2.2

/-- `hist n` is the (observable) history up to time `n`. -/
def hist (n : ℕ) (ω : ℕ → α × E × R) : Iic n → α × R :=  fun i ↦ (action i ω, reward i ω)

/-- `env` is the "environment" distributed according to `Q` that, together with the kernel `k`,
defines how the rewards are generated given actions. -/
def env (ω : ℕ → α × E × R) : E := (ω 0).2.1

/-- The posterior over "environments" for every given history (for a fixed algorithm). -/
noncomputable
def posterior [StandardBorelSpace E] [Nonempty E] (alg : Algorithm α R) (n : ℕ) :
    Kernel (Iic n → α × R) E :=
  condDistrib env (hist n) (trajMeasure Q κ alg)

instance [StandardBorelSpace E] [Nonempty E] (alg : Algorithm α R) (n : ℕ) :
    IsMarkovKernel (posterior Q κ alg n) := by
  unfold posterior
  infer_instance

lemma isAlgEnvSeq_trajMeasure
    [StandardBorelSpace α] [Nonempty α] [StandardBorelSpace R] [Nonempty R] [StandardBorelSpace E]
    [Nonempty E] (alg : Algorithm α R) :
    IsAlgEnvSeq action IT.reward (alg.prod_left E) (StationaryEnv Q κ) (trajMeasure Q κ alg) :=
  IT.isAlgEnvSeq_trajMeasure _ _

end Learning.Bayes
