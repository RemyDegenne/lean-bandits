\chapter{Stochastic multi-armed bandits}

\section{Algorithm, bandit and probability space}

A bandit algorithm is an algorithm in the sense of Definition~\ref{def:algorithm}.
We call the actions \emph{arms} and the observations \emph{rewards}.

The first arm pulled by the algorithm is sampled from $P_0$, the arm pulled at time $1$ is sampled from $\pi_0(H_0)$, where $H_0 \in \mathcal{A} \times \mathcal{R}$ is the data of the first arm pulled and the first observation received, and so on.


\begin{definition}[Bandit]\label{def:bandit}
  \uses{def:stationaryEnv}
  \leanok
A stochastic bandit is simply a reward distribution for each arm: a Markov kernel $\nu : \mathcal{A} \rightsquigarrow \mathbb{R}$, conditional distribution of the rewards given the arm pulled.
It is a stationary environment in which the observation space is $\mathcal{R} = \mathbb{R}$.
\end{definition}

An algorithm can interact with a bandit to produce a sequence of arms and rewards: after a time $t$, the history $H_t \in (\mathcal{A} \times \mathcal{R})^{t+1}$ contains the arms pulled and rewards received up to that time,
\begin{itemize}
  \item the algorithm chooses an arm $A_{t+1}$ sampled according to its policy $\pi_t(H_t)$,
  \item the bandit generates a reward $R_{t+1}$ according to the distribution $\nu(A_{t+1})$,
  \item the history is updated to $H_{t+1} = ((A_0, R_0), \ldots, (A_{t+1}, R_{t+1}))$.
\end{itemize}

We now want to define a probability space on which we can study the sequences of arms and rewards, and formulate probabilistic statements about the interaction between the algorithm and the bandit.


\inputleannode{def:Bandit.measure}


\section{The array model of rewards}

We previously built a probability space on which we can define the sequence of arms and rewards generated by the interaction between the algorithm and the bandit, using the Ionescu-Tulcea theorem.
From Theorem~\ref{thm:isAlgEnvSeq_unique}, we know that the law of the sequence of arms and rewards is independent of the probability space used to define them.
Nonetheless, we now build an alternative model of the rewards, on which it will be easier to prove concentration inequalities.
By the uniqueness of the law, these statements will then transfer to any algorithm-environment interaction.

In the ``array model'', we consider a probability space on which we have an infinite array of rewards from each arm, independent from each other.
When pulling an arm, the algorithm sees the next previously unseen reward from that arm in the array.

\inputleannode{def:arrayMeasure}


\inputleannode{def:algFunction}


\inputleannode{def:AM.history}


The goal of this section is to show that $(\Omega_{\mathcal{A}}, P_{\mathcal{A}})$ with the actions and rewards defined above is an algorithm-environment sequence as in Definition~\ref{def:IsAlgEnvSeq}.


\subsection{Measurability}

TODO: some of those results are proved for $\mathcal{A}$ countable. Add that assumption where needed.

\begin{remark}[Proving measurability with respect to a sub-sigma-algebra]
In this section, we often need to prove that a random variable $X$ is measurable with respect to the sigma-algebra generated by a subset of the independent random variables defining the probability space $\Omega_{\mathcal{A}}$.
We want to prove that $X : \Omega_{\mathcal{A}} \to \mathcal{X}$ is measurable with respect to the sigma-algebra $\sigma((\omega_p)_{p \in S})$, where $S$ is a subset of the indices of the independent random variables defining $\Omega_{\mathcal{A}}$.
In most cases, this is due to $X$ being defined (possibly recursively in a complicated way) only using those random variables.
However, it might be difficult to exhibit an explicit function $f$ such that $X = f((\omega_p)_{p \in S})$.

Here is a general strategy to prove such measurability results in Lean:
\begin{enumerate}
  \item Prove that $X$ is measurable with respect to the full sigma-algebra on $\Omega_{\mathcal{A}}$.
  \item Prove a congruence lemma: for any $\omega, \omega' \in \Omega_{\mathcal{A}}$, if $\omega_p = \omega'_p$ for all $p \in S$, then $X(\omega) = X(\omega')$.
  \item Define $g : (\prod_{p \in S} \Omega_p) \to \Omega$ by $g((\omega_p)_{p \in S}) = \omega'$, where $\omega'_p = \omega_p$ for $p \in S$ and $\omega'_p$ is some fixed value for $p \notin S$.
  \item Write $X = X \circ g \circ \mathrm{proj}_S$, where $\mathrm{proj}_S : \Omega_{\mathcal{A}} \to \prod_{p \in S} \Omega_p$ is the projection on the coordinates in $S$.
  \item Conclude that $X$ is the composition of a measurable function $(X \circ g)$ and the random variable generating the sub-sigma-algebra, and thus is measurable with respect to that sub-sigma-algebra.
\end{enumerate}
\end{remark}

\inputleannode{lem:AM.measurable_hist}




\inputleannode{lem:AM.hist_congr}




\inputleannode{lem:AM.stepsUntil_congr}




\inputleannode{def:AM.probSpaceSubsets}


\inputleannode{lem:AM.measurable_hist_todo}




\inputleannode{lem:AM.measurable_action_add_one_truePast}




\inputleannode{lem:AM.measurable_pullCount_add_one_truePast}




\inputleannode{lem:AM.measurable_stepsUntil}




\inputleannode{lem:AM.measurable_pullCount_action_add_one_hist}




\subsection{Independence}


\inputleannode{lem:AM.indepFun_fst_add_one_aux}




\inputleannode{lem:AM.indepFun_fst_add_one_hist}




\inputleannode{lem:AM.indepFun_snd_apply_aux}




\inputleannode{lem:AM.indepFun_snd_apply_pullCount_action}




\inputleannode{lem:AM.indepFun_snd_hist_cond}




\subsection{Laws}


\inputleannode{lem:AM.hasLaw_action_zero}




\inputleannode{lem:AM.hasCondDistrib_reward_zero}




\inputleannode{lem:AM.hasCondDistrib_action}




\inputleannode{lem:AM.hasCondDistrib_reward_pullCount_action}




\inputleannode{lem:AM.hasCondDistrib_reward_hist_action_pullCount}




\inputleannode{lem:AM.condIndepFun_reward_hist}




\inputleannode{lem:AM.hasCondDistrib_reward}



\inputleannode{thm:isAlgEnvSeq_arrayMeasure}






\section{Law of the n\textsuperscript{th} pull}\label{sec:alt_model}

This section describes the law of $Y_{n, a}$ the $n^{th}$ reward obtained from an arm $a \in \mathcal{A}$ (Definition~\ref{def:rewardByCount}).

We augment the probability space $\Omega$ on which we have an algorithm-environment sequence with $\Omega' = \mathbb{R}^{\mathbb{N} \times \mathcal{A}}$, on which we put the product measure $\bigotimes_{n \in \mathbb{N}, a \in \mathcal{A}} \nu(a)$.
With that measure, the law of $Z_{n,a}$ is $\nu(a)$.


\inputleannode{lem:measurable_comap_indicator_stepsUntil_eq}




\inputleannode{lem:condIndepFun_reward_stepsUntil_arm}




\inputleannode{lem:reward_cond_stepsUntil}




\inputleannode{lem:condDistrib_ae_eq_cond}




\inputleannode{lem:condDistrib_rewardByCount_stepsUntil}




\inputleannode{lem:hasLaw_rewardByCount}




% \begin{lemma}\label{lem:indepFun_rewardByCount_stepsUntil}
%   \uses{def:stepsUntil, def:rewardByCount, def:Bandit.measure}
% For $n > 0$, $Y_{n, a} \ind T_{n,a}$.
% \end{lemma}

% \begin{proof}
%   \uses{lem:condDistrib_rewardByCount_stepsUntil, lem:hasLaw_rewardByCount}
% It suffices to prove that $\mathcal{L}(Y_{n,a} \mid T_{n,a}) = \mathcal{L}(Y_{n,a})$.

% By Lemma~\ref{lem:hasLaw_rewardByCount}, $\mathcal{L}(Y_{n,a}) = \nu(a)$.
% By Lemma~\ref{lem:condDistrib_rewardByCount_stepsUntil}, $\mathcal{L}(Y_{n,a} \mid T_{n,a}) = \nu(a)$.
% \end{proof}


% \begin{lemma}\label{lem:condIndepFun_rewardByCount_hist}
%   \uses{def:rewardByCount, def:stepsUntil, def:Bandit.measure}
% For $n > 0$, $Y_{n, a} \ind H_{T_{n,a}-1} \mid T_{n,a}$, in which $H_{\infty}$ is interpreted as the whole history.
% \end{lemma}

% \begin{proof}
%   \uses{lem:condDistrib_rewardByCount_stepsUntil, lem:condIndepFun_reward_hist_action}
% By Lemma~\ref{lem:condDistrib_rewardByCount_stepsUntil}, $\mathcal{L}(Y_{n,a} \mid T_{n,a}) = \nu(a)$.
% We need to prove that $\mathcal{L}(Y_{n,a} \mid H_{T_{n,a}-1}, T_{n,a}) = \nu(a)$.
% By Lemma~\ref{lem:stepsUntil_basic}, if $T_{n,a} = t \in \mathbb{N}$, then $A_t = a$.
% We get
% \begin{align*}
%   \mathcal{L}(Y_{n,a} \mid H_{T_{n,a}-1}, T_{n,a} = t)
%   &= \mathcal{L}(R_t \mid H_{t-1}, T_{n,a} = t, A_t = a)
%   \: .
% \end{align*}
% Then, $\mathbb{I}\{T_{n,a} = t\}$ is a function of $(H_{t-1}, A_t)$ by Lemma~\ref{lem:measurable_comap_indicator_stepsUntil_eq}, such that

% \begin{align*}
%   \mathcal{L}(R_t \mid H_{t-1}, T_{n,a} = t, A_t = a)
%   &= \mathcal{L}(R_t \mid H_{t-1}, A_t = a)
%   \: .
% \end{align*}
% Thus, using Lemma~\ref{lem:condIndepFun_reward_hist_action}, we have
% \begin{align*}
%   \mathcal{L}(R_t \mid H_{t-1}, A_t = a)
%   = \nu(a)
%   \: .
% \end{align*}

% If $T_{n,a} = \infty$, then $\mathcal{L}(Y_{n,a} \mid H_{T_{n,a}-1}, T_{n,a} = \infty) = \mathcal{L}(Z_{n,a} \mid H_{\infty}, T_{n,a} = \infty) = \nu(a)$ by independence of $Z_{n,a}$ from the history and $T_{n,a}$.

% \end{proof}


% \begin{lemma}\label{lem:indepFun_rewardByCount_hist_stepsUntil}
%   \uses{def:rewardByCount, def:stepsUntil, def:Bandit.measure}
% For $n > 0$, $Y_{n, a} \ind (H_{T_{n,a}-1}, T_{n,a})$.
% \end{lemma}

% \begin{proof}
%   \uses{lem:condIndepFun_rewardByCount_hist}
% By Lemma~\ref{lem:condIndepFun_rewardByCount_hist}, $Y_{n, a} \ind H_{T_{n,a}-1} \mid T_{n,a}$.
% By Lemma~\ref{lem:indepFun_rewardByCount_stepsUntil}, $Y_{n, a} \ind T_{n,a}$.
% By the contraction property of conditional independence (Lemma~\ref{lem:indepFun_contraction}), we have $Y_{n, a} \ind (H_{T_{n,a}-1}, T_{n,a})$.
% \end{proof}


% \begin{lemma}\label{lem:iIndepFun_rewardByCount}
%   \uses{def:rewardByCount}
% The rewards $(Y_{n,a})_{n \in \mathbb{N}}$ are independent.
% \end{lemma}

% \begin{proof}
%   \uses{lem:iIndepFun_nat_iff_forall_indepFun, lem:indepFun_contraction, lem:indepFun_rewardByCount_stepsUntil}
% By Lemma~\ref{lem:iIndepFun_nat_iff_forall_indepFun}, it suffices to show that for all $n \in \mathbb{N}$, $Y_{n+1, a}$ is independent of $(Y_{1,a}, \ldots, Y_{n,a})$.

% By the contraction property of conditional independence (Lemma~\ref{lem:indepFun_contraction}), it suffices to show that $Y_{n+1, a}$ is independent of $(Y_{1,a}, \ldots, Y_{n,a})$ conditionally on $T_{n+1, a}$ and that $Y_{n+1, a}$ is independent of $T_{n+1, a}$.

% The fact that $Y_{n+1, a}$ is independent of $T_{n+1, a}$ is Lemma~\ref{lem:indepFun_rewardByCount_stepsUntil}.

% TODO

% \end{proof}


% \begin{lemma}\label{lem:independent_rewardByCount}
%   \uses{def:rewardByCount}
% For $a \in \mathcal{A}$, let $Y^{(a)} = (Y_{n,a})_{n \in \mathbb{N}} \in \mathbb{R}^{\mathbb{N}}$ be the sequence of rewards obtained from pulling arm $a$. Then the sequences $(Y^{(a)})_{a \in \mathcal{A}}$ are independent.
% \end{lemma}

% \begin{proof}

% \end{proof}


% \begin{lemma}\label{lem:identDistrib_rewardByCount_stream}
%   \uses{def:rewardByCount}
% The random sequences $(Y_{n+1,a})_{n \in \mathbb{N}}$ and $(Z_{n,a})_{n \in \mathbb{N}}$ are identically distributed.
% \end{lemma}

% \begin{proof}
%   \uses{lem:hasLaw_rewardByCount, lem:iIndepFun_rewardByCount}

% \end{proof}


% \begin{lemma}\label{lem:identDistrib_sum_Icc_rewardByCount}
%   \uses{def:rewardByCount}
% The random variables $\sum_{i=1}^n Y_{i,a}$ and $\sum_{i=0}^{n-1} Z_{i,a}$ are identically distributed.
% \end{lemma}

% \begin{proof}
%   \uses{lem:identDistrib_rewardByCount_stream}
% Immediate consequence of Lemma~\ref{lem:identDistrib_rewardByCount_stream}.
% \end{proof}


\section{Regret and other bandit quantities}

\begin{definition}[Arm means]\label{def:armMean}
  \uses{def:bandit}
  \leanok % no actual Lean def, but we don't need one
For an arm $a \in \mathcal{A}$, we denote by $\mu_a$ the mean of the rewards for that arm, that is $\mu_a = \nu(a)[\mathrm{id}]$.
We denote by $\mu^*$ the mean of the best arm, that is $\mu^* = \max_{a \in \mathcal{A}} \mu_a$.
\end{definition}


\inputleannode{def:regret}


\inputleannode{def:gap}


\inputleannode{lem:sum_pullCount_mul}




\inputleannode{lem:regret_eq_sum_pullCount_mul_gap}


